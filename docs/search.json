[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lego I",
    "section": "",
    "text": "Lego I é uma introdução à Ciência Social quantitativa. O curso apresenta os fundamentos da análise de dados, incluindo noções sobre desenho de pesquisa, probabilidade e inferência, testes de hipóteses e modelos de regressão linear. Complementarmente, o curso também introduz o uso de programação em R, um ambiente amplamente utilizado para análise de dados e Data Science dentro e fora da academia. A dinâmica do curso combinará tutoriais e sessões guiadas de monitoria nas quais resolveremos problemas aplicados de pesquisa social de forma reprodutível. [ementa]\n\nProfessor: Bruno Marques Schaefer (IESP-UERJ) [homepage]\nSemestre: 2025.1",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>index.html</span>"
    ]
  },
  {
    "objectID": "aulas/aula-1.html",
    "href": "aulas/aula-1.html",
    "title": "2  Fundamentos da pesquisa quantitativa",
    "section": "",
    "text": "2.1 Anotações das leituras",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fundamentos da pesquisa quantitativa</span>"
    ]
  },
  {
    "objectID": "aulas/aula-1.html#anotações-das-leituras",
    "href": "aulas/aula-1.html#anotações-das-leituras",
    "title": "2  Fundamentos da pesquisa quantitativa",
    "section": "",
    "text": "2.1.1 Kellstedt, P. M., & Whitten, G. D. (2018). The fundamentals of political science research. Cambridge University Press., Cap. 1 e 2.\nCapítulo 1: O estudo científico da política\n\nEntão, o que fazem os cientistas políticos e o que faz deles cientistas? Uma resposta simples para essa questão é que, como outros cientistas, cientistas políticos desenvolvem e testam teorias. Uma teoria é uma tentativa de conjeturar sobre as causas de um fenômeno de interesse. O desenvolvimento de teorias causais sobre o mundo político requer pensar em fenômenos familiares de modo novo. Assim, a construção de uma teoria é em parte arte e em parte ciência.\n\n\nEntão, como cientistas políticos desenvolvem teorias sobre política? Um elemento-chave desse procedimento é como eles organizam seus pensamentos sobre o mundo político em termos de conceitos que cientistas chamam de variáveis e de relações causais entre variáveis. (p. 33)\n\n\nUma vez que tenhamos descoberto nosso relacionamento hipotético, podemos coletar dados de casos do mundo real e observar como esses dados refletem nossas expectativas em relação positiva ou negativa. (p. 38)\n\n\nO ponto essencial é que modelos são simplificações. Se eles são ou não úteis para nós depende do que estamos tentando alcançar com um determinado modelo. Um dos aspectos mais marcantes dos modelos é que eles frequentemente são mais úteis para nós quando são imprecisos do que quando são precisos. O processo de pensar sobre as falhas de um modelo para explicar um ou mais casos pode gerar novas teorias causais. Encontrar imprecisões, frequentemente, nos aponta a direção do progresso teórico frutífero. (p. 41)\n\nCapítulo 2: A arte da construção de teorias\n\n[…] uma boa teoria científica não explica apenas o resultado das eleições presidenciais de 2012, mas das eleições presidenciais americanas em geral. Isto é, em vez de perguntar “Por que Obama venceu Romney em 2012?”, devemos perguntar “O que faz o partido do presidente vencer ou perder uma eleição presidencial nos EUA?” ou “O que faz com que candidatos do Partido Republicano se saiam melhor ou pior que candidatos do Partido Democrata nas eleições presidenciais americanas?”. (p. 56)\n\n\nAs teorias que temos considerado até o momento foram elaboradas após um rigoroso processo de pensamento sobre o fenômeno que queremos explicar e a dedução de possíveis explicações causais. Uma extensão desse tipo de pensamento rigoroso é conhecida por “teoria formal” ou “escolha racional”. Pesquisadores têm utilizado essa abordagem para desenvolver respostas para perguntas de pesquisas sobre como pessoas tomam decisões estratégicas. Colocando de outro modo, se a política é um jogo, como explicamos o modo como as pessoas jogam? (p. 59)\n\n\nPodemos sumarizar esses pressupostos a respeito do comportamento humano dizendo que teóricos formais assumem que todos os indivíduos são maximizadores racionais de utilidade - que eles tentam maximizar seu próprio interesse. (p. 60)\n\n\nUsamos as ferramentas da teoria formal para tentar nos colocar na posição de imaginar que estamos no lugar de outra pessoa e pensar sobre as diferentes escolhas que ele ou ela tiveram que fazer. Nas seções seguintes introduziremos as ferramentas básicas para fazer isso utilizando a abordagem de utilidade esperada […]. (p. 60)\n\n\nA utilidade para uma ação em particular é igual à soma de todos os benefícios menos a soma de todos os custos de uma ação. Sendo \\(Y\\) uma ação, podemos sumarizar a utilidade como: \\[\nU_i (Y) = \\sum B_i (Y) - \\sum C_i (Y)\n\\] Quando escolhe entre um conjunto de ações possíveis, um indivíduo racional escolherá a ação que maximiza sua utilidade. (p. 61)\n\n\nOutro aspecto potencialmente problemático do pressuposto do ator racional maximizador de utilidade que você tem que considerar é o pressuposto da informação completa. Em outras palavras, e se não soubermos exatamente quais são os custos e benefícios de uma ação específica? […]. Quando relaxamos esse pressuposto, deslocamos nossa discussão da utilidade para a utilidade esperada. (p. 62)\n\n\n\n2.1.2 Chen, Yunsong, et al. “Social prediction: a new research paradigm based on machine learning”. The Journal of Chinese Sociology 8 (2021): 1-21.\n\nThus, sociologists have considered it their duty to describe and explain social processes and phenomena, seek the meaning and interpretation of society and social actions, or verify social hypothesis and theories. Consequently, description, interpretation, and statistical testing are traditional and mainstream approaches in sociological research. (p. 1)\n\n\nHowever, scholars have already emphasized that a causal interpretation must serve as the “basis for predicting social phenomena” (Hempel and Oppenheim 1948:138). Hence, predictability is a necessary but not sufficient condicional for the causal mechanism, as although predictability does not equate with causality, a prediction could be formed if there was causality. (p. 1)\n\n\nHowever, sociology and even the broader category of social science pay much less attention to ex ante prediction than ex post facto evalutation and interpretation. A series of papers on the topic of “prediction and its limits” have been published in Science. Although these articles came from different disciplines of social sciences such as economics, sociology, and political science, scholars’ consensus was that in contrast to natural science, theories and data of social science were rarely used to make predictions. (p. 2)\n\n\nwe suggest that social prediction represents a new subparadigm of social science research from the methodological perspective rather than strict ontology. With the emergence of big data and the improvement of compution power, the application of machine learning and the redefining of social prediction will boost the paradigm breakthrough of quantitative research in sociology and even social science research. (p. 3)\n\n\nAs early as in the 1940s, scholars suggested strengthening the work of prediction in the social science and proposed the concept of social prediction (Kaplan 1940). Although Kaplan (1940) realized the dificulty of prediction, he believed that social behavior was even more predictable than natural phenomena at the microlevel. (p. 3)\n\n\nSince the data, models, and computing power required for accurate prediction were not ready, the whole social science community, especially quantitative research, has focused on the relationships among variables; that is, based on limited sample data, unbiased estimations for the pairwise relationships between variables are obtained by statistical models. (p. 4)\n\n\nAlthough sociologists are still upset and worried about the invasion of advanced econometric methods into the discipline, scholares at the forefront of the discipline have given a clear and determined answer that the covariant research of correlation analysis and causal analysis is not enough to constitute a sociological explanation in terms of science. Duncan Watts pointed out the paradigm crisis of sociological research, namely, overreliance on common sense. Many sociological explanations confuse understandability with causality, which does not meet the standard of scientific interpretation. If sociologists expect their interpretations to be scientifically legitimate, they must evaluate them according to scientific standards; that is, predictions must be made (watts 2014:313). (p. 5)\n\n\nIn summary, prediction is the main component of realizing the scientific research goal of quantitative sociology. Among correlation, causality, and prediction, correlation is the prerequisite for causality and prediction, while causality is a sufficient but unnecessary condition for prediction. (p. 5)\n\n\nSpecifically, the error of a linear fitting of the model can be divided into three parts: bias, variance, and irreducible error. These errors represent the deviation between the fitting expectation and the real value, the dispersion of the fitting value, and the inevitable system noise, respectively. (p. 8)\n\n\nBased on our understanding of sociology and some of the latest literature, we classify the disciplinary value of social prediction into three dimensions: academic significance, governance significance, and discourse significance1. (p. 10)\n\n\nThe academic significance of social prediction: (i) Latent indicators of interest can be obtained through prediction (that is, people are unwilling to disclose their own information, but they can be predicted through other information); (ii) theoretical hypotheses can be generated via prediction; (iii) prediction helps to conduct causal inference; (iv) data proliferation can be realized by prediction; (v) the prediction could promote theoretical innovation.\n\n\nThe governance significance of social prediction: (i) one can make use of prediction competitions to promote the development of social prediction; (ii) social inequality research; (iii) public health governance.\n\n\nWith the rapid development of social prediction based on machine learning, we suggest that the basic paradigm of empirical work will be split into three pieces from the original dual peaks of qualitative and quantitative work to a trilogy of qualitative work, quantitative work, and quantitative prediction. […].\nFirst, in terms of epistemology, the black box mechanism is introduced into prediction. […]. Second, in terms of the problem domain, the prediction does not focus on the correlation and causal mechanism of the cause and result, but takes the accurate estimation of the target variables as the goal. Third, in terms of the research method, the prediction reduces the dependence on theory and the focus on the counterfactual framework and instead relies on algorithms and data to train and test models.\n\n\nIn this sense, the black box mechanism will always accompany social prediction. However, in the process of dismantling the black box, the conclusions drawn from the black box mechanism can and should be interpreted from the theoretical perspective and further verified by empirical methods. (p. 18)",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fundamentos da pesquisa quantitativa</span>"
    ]
  },
  {
    "objectID": "aulas/aula-1.html#anotações-de-aula",
    "href": "aulas/aula-1.html#anotações-de-aula",
    "title": "2  Fundamentos da pesquisa quantitativa",
    "section": "2.2 Anotações de aula",
    "text": "2.2 Anotações de aula\nNas ciências sociais, passamos por uma série de dificuldades na produção de conhecimento científico. Em particular, nosso objeto de estudo é difícil (para não dizer impossível) de ser “isolado”, de maneira que a realização de experimentos é muito mais limitada. Outro desafio diz respeito ao fato de que as pessoas, no senso comum, têm explicações para os fenômenos sociais (sociologia espontânea, como diria Bourdieu).\nNas ciências sociais, nos diferenciamos de várias maneiras. Temos uma definição própria do que é sociedade (ou política); usamos uma linguagem “especial”; e usamos o método científico.\nMas, no fim das contas, o que é método científico? No caso Popperiano, literatura e teoria \\(\\rightarrow\\) hipótese \\(\\rightarrow\\) observação \\(\\rightarrow\\) rejeitar ou falhar em rejeitar a hipótese. Por outro lado, sob esse paradigma de ciência, várias coisas que fazemos nas ciências sociais não são consideradas ciência – em particular, a Antropologia, várias áreas da Sociologia etc2.\n\n2.2.1 Metodologia quantitativa\nA pesquisa quantitativa e a qualitativa partem de pressupostos epistemológicos diferentes. A pesquisa quantitativa parte de uma visão mais naturalista (ou positivista3) de que eu consigo extrair um “pedaço” do mundo político e fazer testes, formular hipóteses etc – isto é, existem uma série de componentes objetivos que podem ser extraídos e estudados. Já a pesquisa qualitativa parte de uma perspectiva epistemológica construtivista: o mundo social e político não se desassocia do pesquisador e, portanto, o mundo social é construído a partir da experiência. De certa forma, essas abordagens informam como fazemos pesquisa.\nPesquisa quantitativa e qualitativa alongam um enorme debate sobre a maneira de abordar objetos nas ciências sociais. Mas, no fim das contas, uma resposta para essa discussão é que a integração dos métodos é um bom caminho – por exemplo, a partir da triangulação de evidências.\nAtravés de alguma estratégia de identificação, tentamos, na pesquisa quantitativa, isolar potenciais explicações para um determinado fenômeno. Nesse caso, queremos entender “o efeito de \\(x\\) sobre \\(y\\)”. Naturalmente, isso acaba sendo diferente de previsão, que também é um dos objetivos da pesquisa quantitativa, embora seja um objetivo preterido e meio adormecido na ciência social moderna.\n\n\n2.2.2 Variações de objetivos\n\nDescrição\nExplicação e causalidade\nPredição\n\n\n\n2.2.3 Desenho de pesquisa\nDefinidos os conceitos (potencialmente abstratos) que desejamos operacionalizar, precisamos operacionalizá-los na forma de variáveis (dependente e independentes) mensuráveis. O conceito por si só já é uma abstração da realidade; as variáveis, portanto, são ainda mais. Mas é a partir daí que podemos efetivamente testar hipóteses.",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fundamentos da pesquisa quantitativa</span>"
    ]
  },
  {
    "objectID": "aulas/aula-1.html#footnotes",
    "href": "aulas/aula-1.html#footnotes",
    "title": "2  Fundamentos da pesquisa quantitativa",
    "section": "",
    "text": "Mais específico do caso chinês.↩︎\nPopper é particularmente radical na sua definição de ciência. Para ser uma ciência, a disciplina deveria ser capaz de criar hipóteses que possam ser testadas e, em princípio, refutadas.↩︎\nPensando em contraste com a pesquisa normativa. Embora o termo “positivismo” possa assumir um caráter pejorativo (Comte e coisa e tal), a ideia é que a pesquisa quantitativa se alinha a uma perspectiva de que é possível fazer generalizações a partir de dados observados.↩︎",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Fundamentos da pesquisa quantitativa</span>"
    ]
  },
  {
    "objectID": "aulas/aula-2.html",
    "href": "aulas/aula-2.html",
    "title": "3  Desenho de pesquisa e mensuração",
    "section": "",
    "text": "3.1 Anotações das leituras",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Desenho de pesquisa e mensuração</span>"
    ]
  },
  {
    "objectID": "aulas/aula-2.html#anotações-das-leituras",
    "href": "aulas/aula-2.html#anotações-das-leituras",
    "title": "3  Desenho de pesquisa e mensuração",
    "section": "",
    "text": "3.1.1 Kellstedt, P. M.,; Whitten, G. D. (2018). The fundamentals of political science research. Cambridge University Press., Caps. 3 e 4\n\n3.1.1.1 Capítulo 3: avaliando relações causais\n\nNossas teorias – que podem estar certas ou erradas – tipicamente especificam que alguma variável independente causa alguma variável dependente. Então nos esforçamos para achar evidêncais empíricas apropriadas para avaliar o quanto uma teoria é ou não suportada (p. 75-76).\n\n\n[Sobre a realidade multivariada do mundo] Então, ebora nossas teorias descrevam a relação proposta entre alguma causa e algum efeito, devemos ter sempre em nossas mentes que o fenômeno que estamos tentando explicar certamente possui outras possíveis causas. E, quando for o momento de desenharmos a pesquisa a fim de testar nossas ideias teóricas […], temos que tentar explicar, ou “controlar para” essas outras causas (p. 76).\n\n\nA noção de causalidade que está no centro dessas disciplinas [das ciências físicas] envolve, principalmente, relações determinísticas – isto é, relações em que, se uma causa ocorre, então o efeito ocorrerá com certeza. Em contrapartida, porém, o mundo das interações humanas consiste em relações probabilísticas – nesse sentido, um aumento em \\(X\\) está associado ao aumento (ou à diminuição) na probabilidade de \\(Y\\) ocorrer, mas essas probabilidades não são certezas. (p. 77).\n\n\nPara qualquer teoria sobre uma relação causal entre \\(X\\) e \\(Y\\), devemos cuidadosamente considerar as respostas para as quatro questões abaixo:\n\nExiste algum mecanismo causal crível que conecta \\(X\\) a \\(Y\\)?\nPodemos eliminar a possibilidade de que \\(Y\\) causa \\(X\\)?\nExiste covariação entre \\(X\\) e \\(Y\\)?\nControlamos por todas as variáveis colineares1 \\(Z\\) que podem tornar a associação entre \\(X\\) e \\(Y\\) espúria? (p. 78).\n\n\n\nDe fato, uma parte substancial dos desacordos entre acadêmicos tem origem no quarto obstáculo causal. A objeção mais frequente quando um acadêmico está avaliando o trabalho de outro acadêmico talvez seja que um pesquisador “falhou em controlar por” alguma variável potencialmente importante da variável dependente. (p. 85)\n\n\n\n3.1.1.2 Capítulo 4: desenho de pesquisa\n\nO objetivo de todos os tipos de desenho de pesquisas é nos ajudar a avaliar quão bem uma teoria se sai perante os quatro obstáculos causais – isto é, responder de modo mais conclusivo possível se \\(X\\) causa \\(Y\\). […] focamos duas das mais comuns e efetivas estratégias usadas por cientistas políticos: experimentos e estudos observacionais. (p. 94)\n\n\nDESENHOS DE PESQUISA EXPERIMENTAIS\nA exposição de uma propaganda com conteúdo negativo sobre um candidato (\\(X\\)) pode, ou não, afetar a probabilidade do eleitor votar no candidato-alvo da propaganda (\\(Y\\)). É importante salientar que a afirmação causal possui um componente direcional específico; isto é, a exposição a propagandas aumentará as chances de o eleitor escolher o candidato que fez a propaganda. (p. 95)\nNós, os pesquisadores, devemos não apenas controlar o valor da nossa variável independente [nesse caso, a exposição à propaganda], mas devemos também atribuir esses valores aos participantes randomicamente. No nosso anúncio de campanha, isso significa que devemos […] usar um gerador de números randômicos ou algum outro mecanismo desse tipo para dividir nossos participantes em um grupo de tratamento (os que assistirão ao anúncio negativo) e um grupo de controle (os que não assistirão ao anúncio, mas algo diferente do tratamento, que nas ciências médicas é chamado de um placebo). (p. 96)\nDe fato, desde que o número de participantes seja razoavelmente grande, a atribuição aleatória de participantes ao grupo de tratamento assegura que os grupos, no seu todo, seja idênticos. Se os dois grupos são idênticos, exceto pelo lançamento da moeda, então podemos ter certeza de que qualquer diferença que observarmos nos grupos deve se dar em razão da variável independente que atribuímos. (p 96)\nAtribuição randômica aos grupos de tratamento e controle ocorre quando os participantes de um experimento são atribuídos randomicamente a diferentes valores de \\(X\\), a variável dependente. É importante observar que a definição não diz nada a respeito de como os indivíduos são selecionados para participar do experimento. Já a amostra aleatória, essencialmente, relaciona-se a como pesquisadores selecionam casos para os seus estudos – eles são selecionados aleatoriamente, o que significa que cada membro da população em questão tem uma mesma probabilidade de ser selecionado. (p. 100)\n[Sobre a limitação de experimentos] Cientistas sociais não podem “atribuir” a pessoas uma identificação partidária ou uma renda, “atribuir” a um país um nível de democratização ou gasto miliar, […]. Essas variáveis simplesmente existem na natureza e não podemos controlar a exposição a elas e atribuir randomicamente valores para diferentes casos (isto é, pessoas ou países). E, no entanto, cientistas sociais se sentem compelidos a estudar esses fenômenos, o que significa que, nessas circunstâncias, devemos empregar um desenho não experimental de pesquisa. (p. 102)\n[Experimentos não têm boa validade externa por usarem uma amostra de conveniência] Reiterando: experimentos não requerem uma amostra que represente a população. De fato, é extremamente rato que experimentos utilizem uma amostra da população. Em experimentos para teste de medicamentos, por exemplo, é comum o uso de anúncios em jornais ou em programas de rádio para convidar pessoas a participar, usualmente envolvendo alguma forma de compensação financeira. (p. 102-103)\n\n\nESTUDOS OBSERVACIONAIS\nMas quais são as opções que estudiosos têm quando não podem controlar a exposição de diferentes valores das variáveis independentes? Em tais casos, a única escolha é recolher dados do mundo como eles são e realizar comparações entre unidades individuais – como pessoas, partidos políticos ou países – ou entre uma quantidade agregada que varia ao longo do tempo. (p. 105)\nIsso leva à definição de um estudo observacional: um estudo observacional é um desenho de pesquisa no qual o pesquisador não tem controle dos valores da variável independente, que ocorrem naturalmente. Todavia, é necessário que exista algum grau de variabilidade na variável independente entre os casos, assim como variação na variável dependente. (p. 105)\n[…] um estudo observacional transversal examina a realidade social transversalmente, focando a variação entre unidades espaciais individuais – novamente, como cidadãos, políticos eleitos, distritos eleitorais ou países – e a explicação da variação da variável dependente entre elas. (p. 109)\n[…] o estudo observacional de séries temporais que tem, em seu núcleo, a comparação ao longo do tempo de uma única unidade espacial. Diferentemente da variação transversal, na qual a relação examinada é entre unidades individuais tipicamente em um único ponto do tempo, em estudos observacionais de séries temporais cientistas políticos tipicamente examinam a variação dentro de uma unidade espacial ao longo do tempo. (p. 109)\nComo os exemplos anteriores demonstraram, quando precisamos controlar outras possíveis causas de \\(Y\\) para superar o quarto obstáculo causal, precisamos controlar por todas elas, não por apenas uma. Mas como sabemos se controlamos por todas as possíveis causas de \\(Y\\)? Em muitos casos, não sabemos com certeza. (p. 110)\n\n\n\n\n3.1.2 King, G., Keohane, R. O., Verba, S. (1994). Designing social inquiry: Scientific inference in qualitative research. Princeton university press., Cap. 1\nO capítulo começa com uma introdução ao debate entre pesquisa quantitativa, mais fortemente associada a medidas, estatísticas e matemática de maneira geral, e a pesquisa qualitativa, focada em análises contextuais e mais profundas. Na prática, o autor argumenta que as diferenças entre as duas abordagens são meramente estilísticas e, portanto, pouco importantes do ponto de vista científico.\n\nAll good research can be understood – indeed, is best understood – to derive from the same underlying log of inference. Both quantitative and qualitative research can be systematic and scientific. (p. 4-5)\n\n\nThis lesson of these efforts [Coercive Cooperation (1992) and Making Democracy Work (1993)] should be clear: neither quantitative nor qualitative research is superior to the other, regardless of the research problem being addressed. (p. 5-6)\n\n\nGood research, that is, scientific research, can be quantitative or qualitative in style. In design, however, scientific research has the following characteristics:\n\nThe goal is inference\nThe procedures are public\nThe conclusions are uncertain\nThe content is the method (p. 7-9)\n\n\nO argumento é que “boa pesquisa” (pesquisa científica) é aquela que segue 4 princípios básicos. O primeiro deles diz respeito ao objetivo, que, afinal, deve ser a inferência. A inferência nesse caso é pensada de maneira mais ampla, podendo ser inferência descritiva ou inferência causal. O segundo princípio diz respeito à publicização dos procedimentos, ou seja, a pesquisa deve ser replicável. O terceiro princípio diz respeito à incerteza das conclusões: inferência é um processo imperfeito, afinal, a coleta de dados propriamente dita já é imperfeita – portanto, inferências precisam ser acompanhadas de medidas de incerteza. Por fim, o quarto princípio afirma que a ciência é, essencialmente, um conjunto de regras e métodos que podem ser utilizados para estudar qualquer assunto.\n\n\n3.1.3 Figueiredo Filho, D. B., Paranhos, R., Rocha, E. C., Silva Jr, J., e Santos, M. D. (2012). Levando Gary King a sério: desenhos de pesquisa em Ciência Política. Revista Eletrônica de Ciência Política, 3(1-2), 86-117.\nPelo menos até o ponto de escrita do artigo, o cenário da formação em métodos de pesquisa (quantitativa e qualitativa) era o “calcanhar de Aquiles” da formação em ciência política no Brasil. Em particular, o artigo começa com uma série de evidências dessa formação deficitária e discute os efeitos disso na produção científica e no mercado de trabalho.\nOs autores fazem uma série de recomendações:\n\n3.1.3.1 Substantivas\n\nExplicitar e justificar a questão de pesquisa: “De forma mais séria, quando a questão de pesquisa é nebulosa, fica difícil, inclusive, de avaliar em que medida os resultados observados respondem satisfatoriamente a indagação proposta pelo trabalho.” (p. 89)\nDescrever os métodos e as técnicas: “Uma pesquisa que não descreve exatamente como os dados foram coletados torna-se irreplicável e, consequentemente, não falsificável.” (p. 91)\nSimplificar a hipótese de trabalho: “Uma hipótese clara tem três componentes básicos: (1) uma relação esperada; (2) uma variável independente e (3) uma variável dependente.” (p. 92). Hipóteses não parcimoniosas apresentam elementos denecessários.\nProduzir inferências causais falsificáveis\nApresentar as limitações do desenho de pesquisa: “As limitações devem ser listadas de forma explícita e pormenorizadas para que a comunidade acadêmica possa avaliar a confiabilidade dos resultados apresentados.” (p. 103)\n\n\n\n3.1.3.2 Procedimentais\n\nMinimizar a complexidade da linguagem\nCompartilhar a base de dados: “Resultados de pesquisa de especialistas que compartilham bases de dados são mais facilmente replicáveis e, portanto, mais facilmente falsificáveis” (p. 106)\nEvitar gráficos nebulosos e tabelas poluídas e incompletas\nSer criticado antes de publicar: “Em síntese, o crivo da comunidade acadêmica é uma das formas mais eficientes de aprimorar desenhos de pesquisa e, posteriormente, resultados de pesquisa. Apenas depois de submeter seu trabalho às críticas, o pesquisador deve submetê-lo aos periódicos especializados.” (p. 111)\nEscolher adequadamente os meios de divulgação",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Desenho de pesquisa e mensuração</span>"
    ]
  },
  {
    "objectID": "aulas/aula-2.html#anotações-de-aula",
    "href": "aulas/aula-2.html#anotações-de-aula",
    "title": "3  Desenho de pesquisa e mensuração",
    "section": "3.2 Anotações de aula",
    "text": "3.2 Anotações de aula",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Desenho de pesquisa e mensuração</span>"
    ]
  },
  {
    "objectID": "aulas/aula-2.html#notas-de-rodapé",
    "href": "aulas/aula-2.html#notas-de-rodapé",
    "title": "3  Desenho de pesquisa e mensuração",
    "section": "3.3 Notas de rodapé",
    "text": "3.3 Notas de rodapé",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Desenho de pesquisa e mensuração</span>"
    ]
  },
  {
    "objectID": "aulas/aula-2.html#footnotes",
    "href": "aulas/aula-2.html#footnotes",
    "title": "3  Desenho de pesquisa e mensuração",
    "section": "",
    "text": "Variável colinear é simplesmente uma variável que está correlacionada com a variável dependente e com a independente de maneira a alterar a relação entre elas.↩︎",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Desenho de pesquisa e mensuração</span>"
    ]
  },
  {
    "objectID": "aulas/aula-5.html",
    "href": "aulas/aula-5.html",
    "title": "4  Indicadores e estatísticas descritivas",
    "section": "",
    "text": "4.1 Anotações das leituras",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Indicadores e estatísticas descritivas</span>"
    ]
  },
  {
    "objectID": "aulas/aula-5.html#anotações-das-leituras",
    "href": "aulas/aula-5.html#anotações-das-leituras",
    "title": "4  Indicadores e estatísticas descritivas",
    "section": "",
    "text": "4.1.1 Kellstedt, P. M.,; Whitten, G. D. (2018). The fundamentals of political science research. Cambridge University Press., Cap. 5.\n\nUma teoria, como dissemos, é meramente uma conjectura sobre a possível relação causal entre dois ou mais conceitos. Como cientistas, devemos sempre resistir à tentação de ver nossas teorias como suportadas de alguma forma antes que tenhamos avaliado evidências do mundo real e até tenhamos feito tudo que podemos com as evidências empíricas para avaliar o quão bem nossa teoria se sai ao tentar superar os quatro obstáculos causais que identificamos no capítulo 3. (p. 116)\n\n\nAs ciências sociais [em relação às ciências físicas], pelo contrário, são ciências novas e o consenso científico sobre como mensurar importantes conceitos é raro. Talvez mais crucial, porém, é o fato de que as ciências sociais lidam com uma dificuldade inerente ao seu objeto em suas previsões: os seres humanos. (p. 118)\n\nProblemas de mensuração podem vir de:\n\nFalta de clareza conceitual (o que exatamente eu quero medir?)\nConfiabilidade (a medida é replicável e consistente?)\nViés de mensuração e confiabilidade (sobre-estimação ou subestimação de modo sistemático dos valores para uma variável)\n\nÉ preferível uma estatística enviesada do que uma não confiável: no primeiro caso, pelo menos, a direção da relação entre as variáveis não se altera\n\n\nPolity IV é uma iniciativa de medir democracia. Incorpora apenas algumas variáveis definidas por Dahl – a parte de “contestação” – e não considera a parte de “participação”.\nSobre o fato de que não fazemos cálculos com variáveis categóricas (ou ordinais):\n\nSe retornarmos para os exemplos da seção anterior, podemos ranquear as cinco categorias da variável “situação financeira familiar no passado” de 1 para a melhor situação até 5 para a pior situação. Mas não nos sentimos muito confiantes em trabalhar com esses valores como normalmente trabalhamos com números. Em outras palavras, podemos dizer que a diferença entre “um pouco pior” e “a mesma coisa” (4-3) é a mesma coisa que entre “muito pior” e “um pouco pior” (5-4)?\n\n\nA única medida de tendência central apropriada para dados categóricos é a moda, definida como o valor que ocorre com mais frequência. (p. 136)\n\nValor médio: \\(\\bar{Y} = \\frac{1}{N} \\sum_{i=1}^{N} Y_i\\)\nA média possui duas propriedades importantes:\n\nPropriedade da soma zero: \\(\\sum_{i=1}^{N} (Y_i - \\bar{Y}) = 0\\) (a soma dos desvios em relação à média é zero)\nPropriedade dos “mínimos quadrados”: \\(\\sum_{i=1}^{N} (Y_i - \\bar{Y})^2 &lt; \\sum_{i=1}^{N} (Y_i - c)^2 \\ \\forall c \\neq \\bar{Y}\\) (a soma dos quadrados dos desvios em relação à média é minimizada)\n\nPor conta dessas propriedades, a média é também o valor esperado da variável. “Se alguém lhe pedisse um palpite para o valor de um caso individual oferecendo como informação apenas o valor médio, baseado nessas duas propriedades da média, o valor médio seria o melhor palpite.” (p. 142)\nVariância: \\(var(Y) = \\frac{1}{N-1} \\sum_{i=1}^{N} (Y_i - \\bar{Y})^2\\). Ela indica, logicamente, a amplitude dos dados ao redor da média.\nUma medida mais intuitiva é o desvio padrão, que é a raiz quadrada da variância: \\(sd(Y) = \\sqrt{Var(Y)}\\). O desvio padrão é expresso na mesma unidade de medida que a variável.\nEm conjunto, essas duas métricas nos dão um resumo numérico da distribuição dos casos ao redor do valor médio da variável.",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Indicadores e estatísticas descritivas</span>"
    ]
  },
  {
    "objectID": "aulas/aula-6.html",
    "href": "aulas/aula-6.html",
    "title": "5  Efeitos causais em experimentos",
    "section": "",
    "text": "5.1 Llaudet, E. and Imai, K. (2022). Data analysis for social science: A friendly and practical introduction. Princeton University Press. Chapter 2: Estimating Causal Effects with Randomized Experiments.\nIf we could observe both potential outcomes for each individual, we could estimate the causal effect of the treatment on the outcome just by taking the difference between the two potential outcomes. That is, we could estimate the causal effect of the treatment on the outcome as:\n\\[\n\\text{Individual Effect}_i = \\Delta Y_i = Y_i(X_i = 1) - Y_i(X_i = 0)\n\\]\nIn fact, we can now compute:\n\\[\n\\hat{\\text{average effect}} = \\bar{Y}_\\text{treatment group} - \\bar{Y}_\\text{control group}\n\\]",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Efeitos causais em experimentos</span>"
    ]
  },
  {
    "objectID": "aulas/aula-6.html#llaudet-e.-and-imai-k.-2022.-data-analysis-for-social-science-a-friendly-and-practical-introduction.-princeton-university-press.-chapter-2-estimating-causal-effects-with-randomized-experiments.",
    "href": "aulas/aula-6.html#llaudet-e.-and-imai-k.-2022.-data-analysis-for-social-science-a-friendly-and-practical-introduction.-princeton-university-press.-chapter-2-estimating-causal-effects-with-randomized-experiments.",
    "title": "5  Efeitos causais em experimentos",
    "section": "",
    "text": "In mathematical notation, we represent the treatment variable as X and the outcome variable as Y. We represent the causal relationship between them visually with an arrow from X to Y. The direction of the arrow indicates that changes in X may produce changes in Y but not the other way around:\n\\(X \\rightarrow Y\\) (p. 28).\n\n\nIn this book, for the sake of simplicity, we focus on treatment variables that are binary, that is, that indicate whether the treatment is present or absent. We define the treatment variable for each individual \\(i\\) as:\n\\[\nX_i = \\begin{cases}\n\\text{1 if individual } i \\text{ receives the treatment} \\\\\n\\text{0 if individual } i \\text{ does not receive the treatment}\n\\end{cases}\n\\]\nBased on whether the individual receives the treatment, we speak of two different conditions: - treatment is the condition with the treatment (\\(X_i = 1\\)); - control is the condition without the treatment (\\(X_i = 0\\)) (p. 28).\n\n\nNote that when estimating a causal effect, we are trying to measure a change in Y, specifically the change in Y caused by a change in X. In mathematical notation, we represent change with \\(\\Delta\\), and, thus, we represent a change in the outcome variable as \\(\\Delta Y\\). (p. 30)\n\n\n\n\nUnfortunately, this kind of analysis is not possible. In the real world, we never observe both potential outcomes for the same individual. Instead, we observe only the factual outcome, which is the potential outcome under whichever condition (treatment or control) was received in reality. We can never observe the conterfactual outcome, which is the potential outcome that would have occurred under whichever condition (treatment or control) was not received in reality. As a result, we cannot compute causal effects at the individual level. (p. 32)\n\n\nFUNDAMENTAL PROBLEM OF CAUSAL INFERENCE: To measure causal effects, we need to compare the factual outcome with the counterfactual outcome, but we can never observe the counterfactual outcome.\n\n\nTo get around the fundamental problem of causal inference, we must find good approximations for the counterfactual outcomes. To accomplish this, we move away from individual-level effecs and focus on the average causal effect accross a group of individuals.\nThe average causal effect of the treatment X on the outcome Y, also known as the average treatment effect, is the average of all individual causal effects of X on Y within a group. Since each individual causal effects is the change in Y caused by a change in X for a particular individual, the average causal effect of X on Y is the average change in Y caused by a change in X for a group of individuals. (p. 33)\n\n\nIn a randomized experiment, also known as a randomized controlled trial (RCT), researchers decide who receives the treatment based on a random process. (p. 35)\n\n\nWhen treatment assignment is randomized, the only thing that distinguishes the treatment group from the control group, besides the reception of the treatment, is chance. This means that although the treatment and control groups consist of different individuals, the two groups are comparable to each other, on average, in all respects other than whether or not they received the treatment. (p. 36)\n\n\nIf the treatment and control groups were comparable before the treatment was administered, however, then we can use the factual outcome of one group as an approximation for the counterfactual outcome of the other. In other words, we can assume that the average outcome of the treatment group is a good estimate of the average outcome of the control group, had the control group received the treatment. Similarly, we can assume that the average outcome of the control group is a good estimate of the average of the treatment group, had the treatment group not received the treatment. As a result, we can approximate the average treatment effect by computing the difference in the average outcomes between the treatment and control groups. (p. 37)\n\n\n\n\nIt is worth repeating that the difference-in-means is a valid estimator of the average causal effect of a treatment on an outcome only when the treatment and control groups are comparable with respect to all the variables that might affect the outcome other than the treatment variable itself. […]. The randomization of treatment assignment enables researchers to isolate the effect of the treatment from the effects of other factors. (p. 38)\n\n\nGiven that we cannot always run experiments, we need to learn how to estimate causal effects in non-experimental settings, using what is called observational data. Unlike experimental data, which refers to data collected from a randomized experiment, observational data are collected about naturally occuring events. Treatment assignment is out of the control of the researchers and is often the result of individual choices.",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Efeitos causais em experimentos</span>"
    ]
  },
  {
    "objectID": "aulas/aula-6.html#other-nice-readings",
    "href": "aulas/aula-6.html#other-nice-readings",
    "title": "5  Efeitos causais em experimentos",
    "section": "5.2 Other nice readings",
    "text": "5.2 Other nice readings\n\nHow research affects policy: experimental evidence from 2.150 Brazilian municipalities [paper] [Nexo]\nExpected discrimination and job search [paper]\nDoes Artificial Intelligence help or hurt gender diversity? Evidence from two field experiments on recruitment in tech [paper]",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Efeitos causais em experimentos</span>"
    ]
  },
  {
    "objectID": "aulas/aula-7.html",
    "href": "aulas/aula-7.html",
    "title": "6  Inferência",
    "section": "",
    "text": "6.1 Llaudet, E. and Imai, K. (2022). Data analysis for social science: A friendly and practical introduction. Princeton University Press. Chapter 3: Inferring Population Characteristics via Survey Research.\nOn prop.tables, we can use the margin argument to make set the reference group (that is, if the values will add up to 1 at the column level or the row level): prop.table(table(dataset$var1, dataset$var2), margin = 1).\nThe mathematical formulation of the standard deviation:\n\\[\n\\text{sd}(X) = \\sqrt{ \\frac{ \\sum_{i=1}^{n} (X_i - \\bar{X})^2 }{n} }\n\\]\n💡When plotting scatterplots, a nice way to analyze the data is to divide the plot in 4 quadrants, according to the mean (or maybe the median) of each variable. As a result, each quadrant will have an objective interpretation (above both means, below both means and so on).\nUnderstanding the correlation formula: first, the z-score, which is the number of standard deviations the observation is above or below the mean:\n\\[\nZ_i^X = \\dfrac{X_i - \\bar{X}}{\\text{sd}(X)}\n\\]\nComputing the correlation requires us to convert the observations of both variables to z-scores. Then, the correlation coefficient is calculated as the average of the products of the z-scores of \\(X\\) and \\(Y\\):\n\\[\n\\begin{align}\n\\text{cor}(X, Y) &= \\dfrac{\\sum^n_{i = 1} Z^X_i \\times Z_i^Y}{n} \\\\\n&= \\dfrac{Z^X_1 \\times Z^Y_1 + Z^X_2 \\times Z^Y_2 + \\cdots + Z^X_n \\times Z^Y_n}{n}\n\\end{align}\n\\]",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferência</span>"
    ]
  },
  {
    "objectID": "aulas/aula-7.html#llaudet-e.-and-imai-k.-2022.-data-analysis-for-social-science-a-friendly-and-practical-introduction.-princeton-university-press.-chapter-3-inferring-population-characteristics-via-survey-research.",
    "href": "aulas/aula-7.html#llaudet-e.-and-imai-k.-2022.-data-analysis-for-social-science-a-friendly-and-practical-introduction.-princeton-university-press.-chapter-3-inferring-population-characteristics-via-survey-research.",
    "title": "6  Inferência",
    "section": "",
    "text": "In survey research, we collect data from a subset of observations in order to understand the target population as a whole. The subset of individuals chosen for study is called a sample. The number of observations in the sample is represented by \\(n\\), and the number of observations in the target population is represented by \\(N\\). (p. 52)\n\n\nA representative sample accurately reflects the characteristics of the population from which it is drawn. Characteristics appear in the sample at similar rates as in the population as whole. (p. 52)\n\n\n[…] the random selection of individuals from the population makes the sample and the target population identical to each other, on average, in both observed and unobserved traits. (p. 53)\n\n\n[On potential challenges] First, to implement random sampling, we need the complete list of observations in the target population. This list is known as the sampling frame. In practice, the sampling frame of a population can be difficult to obtain. Lists of residential addresses, emails, or phone numbers often do not include the entire population of interest. More problematically, the individuals missing tend to be systematically different from those included. (p. 54)\n\n\nBefore continuing with the analysis, it is worth noting that removing observations with missing values from a dataset might make the remaining sample of observations unrepresentative of the target population, thereby rendering our inferences about the population characteristics invalid. Here, for example, if respondents who refused to prove their level of education were all in favor of Brexit, our analysis of the new dataframe, bes1, would undermine the level of support for Brexit. (p. 62)\n\n\n\nAnother option [other than histograms and tables] for measuring the differences between Brexit supporters and non-supporters in terms of age distribution is to compute and compare descriptive statistics. Descriptive statistics numerically summarize the main traits of the distribution of a variable.\nWe can use two different types of descriptive statistics:\n\nMeasures of centrality, such as the mean and the median, summarize the center of the distribution.\nMeasures of spread, such as the standard deviation and the variance, summarize the amount of variation of the distribution relative to its center. (p. 71-72)\n\n\n\n\n\nRoughly speaking, the standard deviation of a variable provides the average distance between the observations and the mean (in the same unit of measurement as the variable). (p. 73)\n\n\nAs we will see in detail later in the book, one of the distinct characteristics of normal distributions is that about 95% of the observations fall within two standard deviations from the mean […]. (p. 75)\n\n\n\nThe correlation coefficient ranges from -1 to 1, and it captures the following two characteristcs of the relationship between two variables:\n\nthe direction of their linear association, that is, the sign of the slope of the line of best fit (which is the line that best summarizes the data)\nthe strength of their association, that is, the degree to which the two variables are linearly associated with each other (p. 82)\n\n\n\nThe correlation is positive whenever the two variables move in the same direction relative to their respective means, that is, when high values in one variable are likely to be associated with high values in the other, and low values in one variable are likely to be associated with low values in the other. (p. 82)\n\n\n\n\n\n\nAs a result, the sign of the correlation coefficient will be:\n\npositive when the two variables tend to move in the same direction relative to their respective means, that is, when above-average values in one variable are usually associated with above-average in the other […], and when below-average values in one variable are usually associated with below-average values in the other. (p. 86)\n\n\n\n[…] if two variables have a correlation coefficient of zero, it does not necessarily mean that there is no relationship between them. Is just means that there is no linear relationship between them. For example, the two variables depicted in the figure in the margin have a strong parabolic relationship. Their correlation is approximately zero, however, because there is no line that would summarize the relationship well. (p. 88)",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferência</span>"
    ]
  },
  {
    "objectID": "aulas/aula-7.html#kellstedt-p.-m.-whitten-g.-d.-2018.-the-fundamentals-of-political-science-research.-cambridge-university-press.-cap.-6-probabilidade-e-inferência-estatística.",
    "href": "aulas/aula-7.html#kellstedt-p.-m.-whitten-g.-d.-2018.-the-fundamentals-of-political-science-research.-cambridge-university-press.-cap.-6-probabilidade-e-inferência-estatística.",
    "title": "6  Inferência",
    "section": "6.2 Kellstedt, P. M.,; Whitten, G. D. (2018). The fundamentals of political science research. Cambridge University Press., Cap. 6: Probabilidade e inferência estatística.",
    "text": "6.2 Kellstedt, P. M.,; Whitten, G. D. (2018). The fundamentals of political science research. Cambridge University Press., Cap. 6: Probabilidade e inferência estatística.\n\nUm evento é o resultado de uma observação aleatória. Dois ou mais eventos podem ser chamados de eventos independentes se a realização de um dos eventos não afeta a realização dos demais. Por exemplo, o lançamento de dois dados representa eventos independentes, porque o lançamento do primeiro dado não afeta o resultado do lançamento do segundo.\nA probabilidade possui algumas propriedades fundamentais. Primeiro, todos os eventos possuem alguma probabilidade de ocorrer e essa chance varia de 0 a 1. Uma probabilidade de valor 0 significa que o evento é impossível, e uma probabilidade com valor igual a 1 significa que o evento acontecerá com absoluta certeza. Por exemplo, se lançarmos dois dados honestos e somarmos as faces voltadas para cima, a probabilidade da soma das faces ser igual a 13 é 0, uma vez que o valor mais alto possível é 12.\nSegundo, a soma de todos os eventos possíveis deve ser exatamente 1. Ou seja, sempre que fazemos uma observação aleatória de um conjunto de eventos possíveis, devemos observar um desses eventos. Por exemplo, se você jogar uma moeda para cima, a probabilidade de o resultado ser cada é de \\(1/2\\), a probabilidade de ser coroa é de \\(1/2\\) e a probabilidade de ser cara ou coroa é de \\(1\\), porque \\(1/2 + 1/2 = 1\\).\nTerceiro, se (mas somente se!) dois eventos forem independentes, então a probabilidade de esses dois eventos ocorrerem é igual ao produto das chances individuais. Então, se você tem uma moeda não viciada e lançá-la três vezes – tenha em mente que cada lançamento é um evento independente –, a chance de o resultado dos três lançamentos ser igual a coroa é de \\(1/2 \\times 1/2 \\times 1/2 = 1/8\\). (p. 154)\n\n\nA distribuição normal […]. Primeiro, ela é simétrica em torno da sua média, de tal modo que a moda, a mediana e a média são iguais. Segundo, a distribuição normal possui áreas abaixo da curva com distâncias específicadas definidas a partir da média. Começando da média e adicionando um desvio padrão para cada uma das direções, temos uma cobertura de 68% de toda a área abaixo da curva. Adicionando mais um desvio padrão, passamos a ter 95% do total da área. Adicionando um terceiro desviopadrão em cada direção, temos 99% da área total da curva capturada.\n\n\nImaginamos que estamos coletando uma amostra de seiscentos lançamentos não uma, mas um número infinito de vezes. Podemos chamar essa hipotética distribuição das médias amostrais de distribuição amostral. Ela é hipotética, porque cientistas quase nunca podem coletar mais de uma amostra para a população subjacente em um determinado ponto do tempo.\nSe seguirmos esse procedimento, podemos obter a média das amostras e as expor graficamente. Algumas estariam acima de 3,50, outras abaixo e algumas seriam exatamente 3,50. Porém, é nesse ponto que temos o resultado-chave: a distribuição amostral terá distribuição normal, embora a distribuição de frequência subjacente, claramente, não seja normal.\nEsse é o insight do teorema do limite central. Se pudéssemos imaginar um número infinito de amostras aleatórias e plotássemos a média de cada uma dessas amostras aleatórias em um gráfico, essas médias amostrais seriam normalmente distribuídas. Adicionalmente, a média da distribuição amostral seria igual à média da verdadeira população. (p. 158-159)\n\n\nMas sabemos que nossa amostra de seinscentos lançamentos pode ser ligeiramente diferente da média verdadeira da população, podendo ser tanto um pouco maior quanto um pouco menor. O que podemos fazer, portanto, é usar nosso conhecimento de que a distribuição amostral é normal e invocar a regra 68-95-99 para criar um intervalo de confiança sobre a provável localização da média da população.\n\n\nÉ possível que estejamos errados e que a média da população esteja fora do intervalo? Sim, e ainda sabemos quão provável é que ela esteja fora do intervalo. Existem 2,5% de chance de que a média da população seja menor que 3,33 e 2,5% de chance de que a média da população seja maior que 3,61, em um total de 5% de chance de que a média da população não esteja no intervalo de 3,33 a 3,61.\n\nO exemplo da aprovação do Obama, explorada por uma pesquisa da NBC News e o Wall Street Journal com mil americanos:\n\\[\n\\bar{Y} = \\dfrac{\\sum (470 \\times 1) + (530 \\times 0)}{1000} = 0.47\n\\]\n\\[\ns_y = \\sqrt{ \\dfrac{470 \\times (1-0.47)^2 + 530 \\times (0 - 0.47)^2}{1000-1} } = 0.5\n\\]\nNosso melhor palpite para a média da população é \\(0.47\\), evidentemente. Além disso, o erro-padrão da média é dado por:\n\\[\n\\sigma_\\bar{Y} = \\dfrac{0.5}{\\sqrt{1000}} = 0.016\n\\]\nCom isso podemos calcular o intervalo de confiança de 95% para a média amostral:\n\\[\n\\bar{Y} \\pm 2 \\times \\sigma_\\bar{Y} = 0.47 \\pm 0.032\n\\]\nEntão a taxa de aprovação do Obama, estimada a partir dos dados do survey, fica entre 43,8% e 50,2%.\n\nSe estivermos interessados em estimar valores populacionais, nos baseando em nossas amostras, com a maior precisão possível, então é desejável ter um intervalo de confiança mais estreito do que alargado.\nComo podemos conseguir isso? Pela fórmula do erro-padrão da média fica claro, utilizando álgebra simples, que podemos obter valores menores para o erro padrão de duas formas: com um numerador menor ou um denominador maior. Como obter um numerador menor – o desvio-padrão da amostra – não é algo que podemos fazer na prática, podemos considerar que é possível ter um denominador maior – isto é, aumentar o tamanho da amostra.\nAmostras grandes reduzirão o tamanho dos erros-padrão e amostras menores aumentarão o tamanho dos erros-padrão.",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferência</span>"
    ]
  },
  {
    "objectID": "aulas/aula-7.html#meireles-f.-e-russo-g.-2022.-pesquisas-eleitorais-no-brasil-tendências-e-desempenho.-estudos-avançados-36-117-131.",
    "href": "aulas/aula-7.html#meireles-f.-e-russo-g.-2022.-pesquisas-eleitorais-no-brasil-tendências-e-desempenho.-estudos-avançados-36-117-131.",
    "title": "6  Inferência",
    "section": "6.3 Meireles, F., e Russo, G. (2022). Pesquisas eleitorais no Brasil: tendências e desempenho. Estudos Avançados, 36, 117-131.",
    "text": "6.3 Meireles, F., e Russo, G. (2022). Pesquisas eleitorais no Brasil: tendências e desempenho. Estudos Avançados, 36, 117-131.\n\nEste artigo analisa as estimativas de mais de 2 mil pesquisas eleitorais com os resultados de cinco eleições municipais e nacionais no Brasil entre 2012 e 2020. Em particular, examinamos como fatores previstos nos planos amostrais, como tamanho da amostra e modo de aplicação de entrevistas, e outros como a distância temporal da data de realização das pesquisas até o dia do pleito, predizem diferenças entre estimativas e resultados oficiais. Entre outros, mostramos que pesquisas de véspera com amostras maiores tiveram resultados mais próximos dos apurados nas urnas no período, assim como pesquisas conduzidas perto do dia dos pleitos. Também documentamos que estimativas de pesquisas em eleições nacionais, especialmente para a Presidência e Governos estaduais em segundo turno, tendem a ser mais próximas dos resultados do que em outras disputas. No geral, portanto, os achados indicam que as pesquisas eleitorais no Brasil têm desempenho similar às realizadas em outros contextos. (Resumo, p. 130)\n\n\nNão obstante a existência de fatores relacionados aos erros em surveys eleitorais, a evidência sistemática de diferentes países mostra que pesquisas nas semanas que antecedem um pleito fornecem, em média, estimativas razoavelmente precisas do que a apuração dos votos revelará – erros têm, ao contrário, apenas diminuído ao longo dos anos (Jennings; Wlezien, 2018). Em certo sentido, dadas as dificuldades envolvidas de se desenhar planos amostrais com informações nem sempre atualizadas e em meio a diferentes fontes não aleatórias de erro, é surpreendente o desempenho das pesquisas eleitorais ao antecipar tendências e resultados, ainda que esse não seja o objetivo ou a função das pesquisas (Gelman, 2021). (p. 118)\n\n\nNossos dados revelam padrões importantes sobre as pesquisas eleitorais no país. Em primeiro lugar, documentamos uma crescente no uso de abordagens telefônicas no período, bem como ligeiro aumento no tamanho das amostras utilizadas ao longo do tempo em eleições nacionais e estaduais. Quanto aos seus resultados, as estimativas eleitorais indicam que o erro médio das nossas pesquisas de véspera é certa de 1.8 ponto percentual (pp), o que pode ser traduzido em uma margem de erro de 6.8 pp, maior do que o convencionalmente reportado por institutos. Isso dito, tal taxa é comparável à de outros países e, mais, há grande variação entre pesquisas: as de presidentes e de governo do estado no segundo turno cometem, em média, erros menores, assim como pesquisas com maiores amostras e em pleitos com menores taxas de eleitores indecisos. (p. 119)\n\n\nVale notar que as pesquisas eleitorais no Brasil têm alguns de seus aspectos regulados pela legislação eleitoral. Entre outros, institutos de pesquisa que queiram divulgar suas estimativas precisam fazer o registro do plano amostral a ser implementado com antecedência, reportando as datas em que a coleta dos dados será feita, a data de divulgação dos resultados, o tamanho da amostra e detalhes de metodologia como uso de quotas e discriminação dos locais onde entrevistas são aplicadas. Em razão disso, por exemplo, margens de erro divulgadas no momento do registro das pesquisas não podem contemplar o efeito do uso de pós-estratificação para corrigir desvios nas amostras, uma vez que o efeito de tais procedimentos só podem ser quantificados após o encerramento da coleta de dados – o que contribui para subestimar possíveis erros aleatórios e não aleatórios, isto é, erros totais (Gramacho, 2013). (p. 121)",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferência</span>"
    ]
  },
  {
    "objectID": "aulas/aula-7.html#anotações-de-aula",
    "href": "aulas/aula-7.html#anotações-de-aula",
    "title": "6  Inferência",
    "section": "6.4 Anotações de aula",
    "text": "6.4 Anotações de aula\nEm estatística, falamos sobre inferência como “realizar afirmações sobre dados que não conhecemos, a partir de dados conhecidos”. No fim das contas, falar sobre uma população a partir de uma amostra. Isso está diretamente associado ao processo gerador dos dados, i.e., o processo subjacente que está gerando os meus dados.\nA população são todos os casos relevantes para a minha análise. Uma amostra, então, é um subconjunto desses casos relevantes.\nA seleção aleatória de elementos da população é a chave da amostragem probabilística:\n\nAusência de viés: amostragem probabilística garante que cada membro da população tenha uma chance de ser incluído na amostra.\nConvergência: a variabilidade natural das amostras se equilibra à medida que \\(N\\) aumenta, levando a média amostral a se aproximar da verdadeira média da população.\n\n\n6.4.1 Lei dos Grandes Números\nLGN é um princípio da teoria da probabilidade que descreve o comportamento da média de uma amostra à medida que ela aumenta. À medida que aumento a amostra, vou me aproximando da média “verdadeira”, i.e., a média da população.\nEm uma amostra pequena, um único sorteio pode distorcer a média amostral. Em uma amostra maior, sorteios extremos se equilibram e levam a média amostram a se aproximar da média da população. Para fins práticos, \\(N \\geq 100\\) reduz bastante a influência de outliers na estimativa da média.\n\nPodemos pensar em 100 retiradas de 10 bolinhas, ou 1000 retiradas de 10 bolinhas. Neste caso, nossa amostra tem o mesmo tamanho, mas expandimos o número de amostras e calculamos a média de cada uma delas. Então, a amostra tem o mesmo tamanho: \\(n = 10\\). A média dessas médias é a média amostral.\n\n\n\n6.4.2 Teorema Central do Limite\nÀ medida que extraímos mais amostras, a distribuição da estimativa começa a assumir a forma de uma distribuição normal.\n\nA distribuição da soma (ou média) das variáveis aleatórias independentes tende a uma distribuição normal à medida que o número de amostras aumenta, independentemente da distribuição original das variáveis (binomial, por exemplo).\n\n\nlibrary(ggplot2)\n\n# cria uma lista com 1000 amostras de 10 bolinhas, podendo elas serem azuis (1) ou vermelhas (0)\nset.seed(123)\nbolinhas &lt;- replicate(1000, sample(c(0, 1), size = 10, replace = TRUE))\n\n# isso aqui tem 1000 amostras de 10 bolinhas\nbolinhas &lt;- as.data.frame(bolinhas)\n\n# calcula a média de cada amostra\nmedias &lt;- apply(bolinhas, 2, mean)\n\n# plota a distribuição das médias\nggplot(data.frame(medias), aes(x = medias)) +\n  geom_histogram(aes(y = ..density..), bins = 30, fill = \"blue\", alpha = 0.5) +\n  # inclui a linha da média amostral (media das medias)\n  geom_vline(aes(xintercept = mean(medias)), color = \"red\", linetype = \"dashed\", size = 1) +\n  stat_function(fun = dnorm, args = list(mean = mean(medias), sd = sd(medias)), color = \"red\") +\n  labs(title = \"Distribuição das Médias Amostrais\",\n       x = \"Média Amostral\",\n       y = \"Densidade\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\n\n\n\n\nEsse é o insight do teorema central do limite. Se pudéssemos imaginar um número infinito de amostras e plotássemos a média de cada uma dessas amostras, essas médias amostrais seriam normalmente distribuídas. Adicionalmente, a média da distribuição amostral seria igual à média da verdadeira população.\nDe fato, outra propriedade interessante é a dispersão da média amostral. A média amostral tende a ser menos dispersa do que a média populacional. Isso é chamado de erro padrão da média (EPM). O EPM é a medida da variabilidade das médias amostrais em torno da média populacional. O EPM é calculado como o desvio padrão da população dividido pela raiz quadrada do tamanho da amostra:\n\\[\nEPM = \\dfrac{\\sigma}{\\sqrt{n}}\n\\]\nonde \\(\\sigma\\) é o desvio padrão da população e \\(n\\) é o tamanho da amostra. O EPM diminui à medida que o tamanho da amostra aumenta. Isso significa que, quanto maior a amostra, mais precisa será a estimativa da média populacional. Isso é importante porque nos permite fazer inferências sobre a população com base em uma amostra.\n\n\n6.4.3 Intervalo de Confiança\nUm intervalo de confiança é uma faixa de valores que, com um certo nível de confiança, contém o valor verdadeiro de um parâmetro populacional. O intervalo de confiança é calculado a partir da média amostral e do erro padrão da média.\nO intervalo de confiança é calculado como: \\[\nIC = \\bar{X} \\pm Z_{\\alpha/2} \\cdot EPM\n\\]\nonde \\(\\bar{X}\\) é a média amostral, \\(Z_{\\alpha/2}\\) é o valor crítico da distribuição normal padrão correspondente ao nível de confiança desejado (por exemplo, 1,96 para um intervalo de confiança de 95%) e \\(EPM\\) é o erro padrão da média.\nO intervalo de confiança fornece uma estimativa da incerteza associada à média amostral. Se o intervalo de confiança for estreito, isso indica que a média amostral é uma boa estimativa da média populacional. Se o intervalo de confiança for amplo, isso indica que a média amostral é uma estimativa menos precisa da média populacional.\n\nO valor de 1.96 é o valor crítico da distribuição normal padrão que corresponde a um nível de confiança de 95%. Isso significa que, se repetirmos o processo de amostragem muitas vezes, aproximadamente 95% dos intervalos de confiança calculados conterão a média populacional verdadeira. Além disso, o intervalo de 95% é arbitrário: por exemplo, 68% de confiança corresponde a 1 desvio padrão, 99% de confiança corresponde a 2.58 desvios padrão, e assim por diante.\n\nVamos supor o caso em que queremos estimar a aprovação do Lula. Pegamos 1000 amostras e estimamos uma média de aprovação de 32%. O desvio padrão, portanto, será calculado como:\n\\[\ns = \\sqrt{ 0.32 \\cdot (1 - 0.32) } = 0.47,\n\\]\ne o erro padrão da média será: \\[\n\\text{EPM} = \\dfrac{0.47}{\\sqrt{1000}} = 0.015\n\\]\nO intervalo de confiança de 95% para a média amostral é dado por: \\[\n\\text{IC} = 0.32 \\pm 1.96 \\cdot 0.015\n\\]\nOu seja, o intervalo de confiança é dado por: \\[\n\\text{IC} = (0.32 - 0.03, 0.32 + 0.03) = (0.29, 0.35)\n\\]\n\n\n\n6.4.4 A variância da Bernoulli\n💡Dedico essa parte das anotações a provar o motivo de a variância da Bernoulli poder ser escrita como \\(p(1-p)\\), sendo \\(p\\) a proporção de \\(X = 1\\) em um conjunto.\nQuando estamos tratando de uma variável aleatória \\(X\\) que segue uma distribuição Bernoulli, sabemos que os valores de \\(X\\) só assumem valores em \\(\\{0, 1\\}\\). Além disso, sabemos que \\(\\mathbb{E}[X] = p\\). A fórmula da variância é:\n\\[\n\\text{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2\n\\]\nNo entanto, sabemos que, como as observações de \\(X\\) só assume valores em \\(\\{0, 1\\}\\), vale que \\(\\mathbb{E}[X^2] = \\mathbb{E}[X] = p\\) e que \\((\\mathbb{E}[X])^2 = p^2\\). Com efeito:\n\\[\n\\begin{align*}\n\\text{Var}(X) &= \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 \\\\\n              &= p - p^2 \\\\\n              &= p(1 - p)\n\\end{align*}\n\\]\nApenas para fixar, vamos provar também o porquê de podermos escrever a variância como \\(\\text{Var}(X) = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2\\).\nA variância, por definição, é a diferença quadrática entre a média de uma variável e o valor observado. Isto é:\n\\[\n\\text{Var}(X) = \\mathbb{E}[(X - \\mathbb{E}[X])^2]\n\\]\nPodemos expandir essa expressão e manipulá-la usando a propriedade da linearidade da esperança:\n\\[\n\\begin{align*}\n\\text{Var}(X) &= \\mathbb{E}[(X - \\mathbb{E}[X])^2] \\\\\n              &= \\mathbb{E}[X^2 - 2X\\mathbb{E}[X] + (\\mathbb{E}[X])^2] \\\\\n              &= \\mathbb{E}[X^2] - 2\\mathbb{E}[X]\\mathbb{E}[X] + (\\mathbb{E}[X])^2 \\\\ % pela linearidade\n              &= \\mathbb{E}[X^2] - 2(\\mathbb{E}[X])^2 + (\\mathbb{E}[X])^2 \\\\\n              &= \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 \\\\\n\\end{align*}\n\\]",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferência</span>"
    ]
  },
  {
    "objectID": "aulas/aula-8.html",
    "href": "aulas/aula-8.html",
    "title": "7  Predição",
    "section": "",
    "text": "7.1 Llaudet, E. and Imai, K. (2022). Data analysis for social science: A friendly and practical introduction. Princeton University Press. Chapter 4: Predicting outcomes using linear regression.\nPrimeiro usamos os dados disponíveis para ajustar (fit) um modelo aos dados. Depois, quando não podemos mais observar os valores da variável dependente (ou variável resposta), usamos o modelo ajustado para estimar os valores de \\(\\hat{Y}\\).\nO modelo linear é dado por:\n\\[\nY_i = \\alpha + \\beta X_i + \\epsilon_i,\n\\]\nonde:\nA reta ajustada é dada por:\n\\[\n\\hat{Y}_i = \\hat{\\alpha} + \\hat{\\beta} X_i,\n\\]\nonde: - \\(\\hat{Y}_i\\) é o valor predito da variável dependente (ou variável resposta); - \\(\\hat{\\alpha}\\) é o valor estimado do intercepto; - \\(\\hat{\\beta}\\) é o valor estimado do coeficiente angular (ou inclinação); - \\(X_i\\) é o valor da variável independente (ou preditora);\nO erro estimado é simplesmente a diferença entre o valor observado e o valor predito:\n\\[\n\\hat{\\epsilon}_i = Y_i - \\hat{Y}_i = Y_i - (\\hat{\\alpha} + \\hat{\\beta} X_i).\n\\]\nDesejamos sempre encontrar a reta que minimiza a soma dos quadrados dos erros.",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Predição</span>"
    ]
  },
  {
    "objectID": "aulas/aula-8.html#llaudet-e.-and-imai-k.-2022.-data-analysis-for-social-science-a-friendly-and-practical-introduction.-princeton-university-press.-chapter-4-predicting-outcomes-using-linear-regression.",
    "href": "aulas/aula-8.html#llaudet-e.-and-imai-k.-2022.-data-analysis-for-social-science-a-friendly-and-practical-introduction.-princeton-university-press.-chapter-4-predicting-outcomes-using-linear-regression.",
    "title": "7  Predição",
    "section": "",
    "text": "In the social sciences, we often are unable to observe the value of a particular variable of interest, \\(Y\\), either because it hasn’t occurred yet or because it is difficult to measure. In these situations, we typically observe the values of the other variable that, if correlated with \\(Y\\), can be used to predict \\(Y\\). (p. 99)\n\n\nWhen analyzing data for predictive purposes, then, we do not assume that there is a causal relationship between \\(X\\) and \\(Y\\); we simply rely on a high degree of correlation between them and use one variable to estimate the value of the other. (p. 99)\n\n\n\n\n\n\n\\(Y_i\\) é a variável dependente (ou variável resposta);\n\\(X_i\\) é a variável independente (ou preditora);\n\\(\\alpha\\) é o intercepto;\n\\(\\beta\\) é o coeficiente angular (ou inclinação);\n\\(\\epsilon_i\\) é o erro aleatório (ou resíduo) associado à observação \\(i\\).\n\\(i\\) é o índice da observação.\n\n\nUnfortunately, we do not know the values of \\(alpha\\), \\(\\beta\\), and \\(\\epsilon_i\\). We need to estimate them based on data. We start by estimating the intercept (\\(\\alpha\\)) and the slope (\\(\\beta\\)), the two coefficients that define the line. This is equivalent to fitting a line to the data, that is, finding the line that best summarizes the relationship between \\(X\\) and \\(Y\\). (p. 102)\n\n\n\n\n\nNote that the value of \\(\\hat{Y}\\) produced by a fitted model is an average predicted value; it is the average predicted value of \\(Y\\) associated with a particular value of \\(X\\). Indeed, predicted outcomes (\\(\\hat{Y}\\)) are equivalent to average outcomes (\\(\\bar{Y}\\)).\n\n\n\n\n\n7.1.1 Os coeficientes\n\no intercepto especifica a posição da reta no eixo vertical. Mais especificamente, o intercepto é o valor de \\(Y\\) quando \\(X = 0\\);\no coeficiente angular especifica a inclinação da reta. Mais especificamente, o coeficiente angular é a variação média de \\(Y\\) associada a uma variação unitária de \\(X\\). Quando \\(\\delta X = 1\\), a variação média de \\(Y\\) é \\(\\hat{\\beta}\\).\n\n\n\n7.1.2 OLS: Ordinary Least Squares\n\nFormally, to choose the line of best fit, we use the “least squares” method, which identifies the line that minimizes the “sum of the squared residuals”, known as SSR.\n\n\\[\n\\text{SSR} = \\sum_{i=1}^{n} = \\hat{\\epsilon}_i^2 = \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i)^2 = \\sum_{i=1}^{n} (Y_i - (\\hat{\\alpha} + \\hat{\\beta} X_i))^2\n\\]\n\nWhy do we want to minimize the sum of the squared residuals rather than the sum of the residuals? Because in the minimization process we want to avoid having positive prediction errors cancel out negative prediction errors. By squaring the residuals, we convert them all to positive numbers. (p. 107)\n\nNo exemplo do livro, os autores ajustam um modelo para prever o GDP dos países a partir da emissão de luzes noturnas. Mas, antes, podemos simplesmente ajustar um modelo que estima o GDP (2005-2006) atual a partir do GDP “anterior” (1992-1993):\n\\[\n\\hat{\\text{gdp}}_{i} = \\hat{\\alpha} + \\hat{\\beta} \\cdot \\text{prior_gdp}_{i}\n\\]\nEstimado o modelo, teríamos os coeficientes:\n\\[\n\\hat{\\text{gdp}}_{i} = 0.72 + 1.61 \\cdot \\text{prior_gdp}_{i}\n\\]\nSuponha que, há alguns anos atrás, o GDP do Brasil era de 400 trihões de dólares. O GPD estimado seria:\n\\[\n\\hat{\\text{gdp}}_{\\text{BR}} = 0.72 + 1.61 \\cdot 400 = 644.72\n\\]\n\n\n7.1.3 With natural logarithm transformations\n\nWhen a variable contains a handful of either extremely large or extremely small values, the distribution of the variable will be skewed. (Recall that a distribution is considered skewed when it is not symmetric because one of its tails is longer than the other.) Under these circumstances, it is often a good idea to transform the variable by taking its natural logarithm. This transformation will make the variable of interest more normally distributed and, in turn, improve the fit of the line to the data. In the example at hand, we will transform both variables of interest by taking the natural logarithm, and then we will re-fit the line. (p. 113)\n\n\nThis type of model, in which both the outcome and the predictor have been log-transformed, is called the log-log linear model. While we could interpret the coefficients the same way as in the normal linear model, in practice, we use an approximation to avoid dealing with the logarithms, especially when interpreting \\(\\hat{\\beta}\\). (p. 116)\nAs shownn in the apprendix near the end of this chapter, we interpret \\(\\hat{\\beta}\\) as the predicted percentage change in the outcome associated with an increase in the predictor of 1 percent. Since here \\(\\hat{\\beta} = 1.01\\), an increase of prior GPD of 1 percent is associated with an increase in GPD of 1.01% on average. Note that in this interpretation of \\(\\hat{\\beta}\\), both the change in \\(X\\) and the change in \\(\\hat{Y}\\) are measured in percentages, instead of in units, as is the case in the standard linear model. In other words, in the log-log model, we estimate change in relative rather than in absolute terms. (p. 116)\n\nNo apêndice do capítulo, os autores discutem matematicamente a interpretação de \\(\\hat{\\beta}\\) no modelo log-log. A fórmula para a variação percentual de \\(Y\\) associada a uma variação percentual de \\(X\\) é dada por:\n\\[\n\\begin{align*}\n\\hat{\\log(Y_\\text{final})} - \\hat{\\log(Y_\\text{inicial})} &= [\\hat{\\alpha} + \\hat{\\beta} \\cdot \\log(X_\\text{final})] - [\\hat{\\alpha} + \\hat{\\beta} \\cdot \\log(X_\\text{inicial})] \\\\\n&= \\hat{\\beta} \\cdot [\\log(X_\\text{final}) - \\log(X_\\text{inicial})] \\\\\n\\end{align*}\n\\]\nDevemos usar a seguinte aproximação:\n\\[\n\\log(Y_\\text{final}) - \\log(Y_\\text{inicial}) \\approx \\frac{Y_\\text{final} - Y_\\text{inicial}}{Y_\\text{inicial}} = \\frac{\\Delta Y}{Y_\\text{inicial}}\n\\]\nIsso é verdade porque, para pequenas variações, a diferença entre o logaritmo de dois números é aproximadamente igual à razão entre a diferença e o número inicial. Em particular, a seguinte afirmação é válida:\n\\[\n\\log(Y + \\Delta Y) - \\log(Y) \\approx \\frac{\\Delta Y}{Y}\n\\]\nNo caso acima, estamos reescrevendo \\(\\log(Y_\\text{final})\\) como \\(\\log(Y + \\Delta Y)\\) Essa é uma propriedade do logaritmo natural derivada da expansão em série de Taylor:\n\\[\n\\log (1+\\epsilon) \\approx \\epsilon - \\frac{\\epsilon^2}{2} + \\frac{\\epsilon^3}{3} - \\cdots \\approx \\epsilon\n\\]\nonde \\(\\epsilon\\) é um número pequeno. Portanto, podemos usar a seguinte aproximação:\nEntendemos, com isso, que a variação percentual de \\(Y\\) associada a uma variação percentual de \\(X\\) é dada por:\n\\[\n\\frac{\\Delta Y}{Y_\\text{inicial}} \\times 100 \\approx \\hat{\\beta} \\cdot \\frac{\\Delta X}{X_\\text{inicial}} \\times 100\n\\]\nMatematicamente, escrevemos:\n\\[\n\\begin{align*}\n[\\log(X_\\text{final}) - \\log(X_\\text{inicial})] \\cdot 100 &= [\\log(X_\\text{inicial} + \\Delta X) - \\log(X_\\text{inicial})] \\cdot 100 \\\\\n&= [\\log (X_\\text{inicial} + X_\\text{inicial} \\frac{\\Delta X}{X_\\text{inicial}}) - \\log(X_\\text{inicial})] \\cdot 100 \\\\\n&= [\\log (X_\\text{inicial} (1 + \\frac{\\Delta X}{X_\\text{inicial}})) - \\log(X_\\text{inicial})] \\cdot 100 \\\\\n&= [\\log (1 + \\frac{\\Delta X}{X_\\text{inicial}})] \\cdot 100 \\\\\n&\\approx \\frac{\\Delta X}{X_\\text{inicial}} \\cdot 100\n\\end{align*}\n\\]\n\n\n7.1.4 Measuring how well the model fits the data with the coefficient of determination, \\(R^2\\)\n\nThe value of \\(R^2\\) ranges from 0 to 1 and represents the proportion of the variation of Y explained by the model. […]. Therefore, the higher the \\(R^2\\), the better the model fits the data. (p. 120)\n\n\\[\nR^2 = 1 - \\frac{\\text{SSR}}{\\text{TSS}}\n\\]\nonde: - \\(SSR\\) é a soma dos quadrados dos resíduos; - \\(TSS\\) é a soma total dos quadrados, que mede a variação total de \\(Y\\) em relação à média de \\(Y\\):\n\\[\nTSS = \\sum_{i=1}^{n} (Y_i - \\bar{Y})^2\n\\]\nNo fim das contas, TSS é simplesmente o numerador da variância.\n\nGiven the definitions above, we can interpret SSR/TSS as the proportion of the variation of \\(Y\\) not explained by the model. Therefore, 1-SSR/TSS is the proportion of the variation of \\(Y\\) that is explained by the model. (p. 121)\n\n💡A interpretação de que o SSR é a variação não explicada é simples: como se trata da diferença entre o que o modelo diz e o que de fato é o valor observado, entendemos que o modelo falhou em explicar essa variação. Além disso, como o TSS representa a variação total em relação à média, podemos usá-lo como denominador.",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Predição</span>"
    ]
  },
  {
    "objectID": "aulas/aula-8.html#verhagen-m.-d.-2022-a-pragmatists-guide-to-using-prediction-in-the-social-sciences.-socius-8-23780231221081702",
    "href": "aulas/aula-8.html#verhagen-m.-d.-2022-a-pragmatists-guide-to-using-prediction-in-the-social-sciences.-socius-8-23780231221081702",
    "title": "7  Predição",
    "section": "7.2 Verhagen, M. D. (2022) A pragmatist’s guide to using prediction in the social sciences. Socius, 8, 23780231221081702",
    "text": "7.2 Verhagen, M. D. (2022) A pragmatist’s guide to using prediction in the social sciences. Socius, 8, 23780231221081702\n\nThe current lack of prediction in the social sciences stems from a seeming incompatibility between wanting to explain and wanting to predict, effectively forcing researchers to choose between the two approaches. A case in point is the much-cited article by Galit Shmueli (2010), aptly titled “To Predict or to Explain”, which outlines how a social scientist’s empirical work flow differs in terms of data processing, modeling, and postestimation diagnostics when choosing to either predict or explain. Naturally, Shmueli assumes that a researcher would not normally attempt to do both. This is an accurate reflection of social science research. The apparent need to dogmatically choose between either approach means that, in practice, social scientists tend to stick to explanation almost exclusively. (p. 1)\n\n\nWhen viewing prediction as a simple tool to evaluate a model’s ability to approximate the outcome of interest, it can be applied without exception to most social science questions, rendering a dogmatic choice between prediction or explanation unnecessary. (p. 2)\n\nO autor faz uma discussão sobre possíveis approaches para a predição – in-sample evaluation, k-fold cross validation, e external evaluation.\nAlém disso, discute também a importância da predição em relação às informações que ela pode trazer sobre os modelos ajustados. Em particular, há 3 virtudes principais:\n\nA predição dá informações sobre o ajuste do modelo aos dados;\nA predição permite o estabelecimento de benchmarks para a avaliação de modelos em diferentes domínios;\nA predição pode ser usada para entender melhor modelos complicados.",
    "crumbs": [
      "Aulas",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Predição</span>"
    ]
  },
  {
    "objectID": "listas/nivelamento/lista-nivelamento.html",
    "href": "listas/nivelamento/lista-nivelamento.html",
    "title": "8  Lista Nivelamento",
    "section": "",
    "text": "9 Material de apoio\nLivro: Usando R: Uma introdução para pesquisadores em Humanidades Digitais, de Fernando Meireles e Denisson Silva\nLink: https://fmeireles.com/livro/",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lista Nivelamento</span>"
    ]
  },
  {
    "objectID": "listas/nivelamento/lista-nivelamento.html#questão-1",
    "href": "listas/nivelamento/lista-nivelamento.html#questão-1",
    "title": "8  Lista Nivelamento",
    "section": "10.1 Questão 1",
    "text": "10.1 Questão 1\nCrie um objeto chamado meu_ano_nascimento e salve nele o ano do seu nascimento. Em seguida, crie um objeto chamado ano_atual e salve nele o ano atual. Por fim, crie um objeto chamado minha_idade e atribua a ele a diferença entre ano_atual e meu_ano_nascimento. Use o console para visualizar o valor de minha_idade.",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lista Nivelamento</span>"
    ]
  },
  {
    "objectID": "listas/nivelamento/lista-nivelamento.html#resposta",
    "href": "listas/nivelamento/lista-nivelamento.html#resposta",
    "title": "8  Lista Nivelamento",
    "section": "10.2 Resposta",
    "text": "10.2 Resposta\nInstanciamos uma variável com o meu ano de nascimento e com o ano atual e, tirando a diferença entre as duas datas, obtemos a minha idade.\n\nano_nascimento &lt;- 2001\nano_atual &lt;- 2025\nminha_idade &lt;- ano_atual - ano_nascimento\nprint(paste(\"Minha idade é:\", minha_idade, \"anos\"))\n\n[1] \"Minha idade é: 24 anos\"",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lista Nivelamento</span>"
    ]
  },
  {
    "objectID": "listas/nivelamento/lista-nivelamento.html#questão-2",
    "href": "listas/nivelamento/lista-nivelamento.html#questão-2",
    "title": "8  Lista Nivelamento",
    "section": "10.3 Questão 2",
    "text": "10.3 Questão 2\nCrie um vetor chamado notas com cinco valores que representam notas que você recebeu em algum curso ou disciplina (use valores de 0 a 10). Calcule a média das notas usando a função mean() e salve o resultado em um objeto chamado media_notas.",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lista Nivelamento</span>"
    ]
  },
  {
    "objectID": "listas/nivelamento/lista-nivelamento.html#resposta-1",
    "href": "listas/nivelamento/lista-nivelamento.html#resposta-1",
    "title": "8  Lista Nivelamento",
    "section": "10.4 Resposta",
    "text": "10.4 Resposta\nSupondo que eu tenha tirado as seguintes notas em um determinado curso, tiramos a média usando a função mean(x) do R.\n\nnotas &lt;- c(2, 4, 6, 8, 10)\nmedia_notas &lt;- mean(notas)\nprint(paste(\"Minha média foi:\", media_notas))\n\n[1] \"Minha média foi: 6\"",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lista Nivelamento</span>"
    ]
  },
  {
    "objectID": "listas/nivelamento/lista-nivelamento.html#questão-3",
    "href": "listas/nivelamento/lista-nivelamento.html#questão-3",
    "title": "8  Lista Nivelamento",
    "section": "10.5 Questão 3",
    "text": "10.5 Questão 3\nPor que o código abaixo não funciona?\nmy_variable &lt;- 10 my_varıable",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lista Nivelamento</span>"
    ]
  },
  {
    "objectID": "listas/nivelamento/lista-nivelamento.html#resposta-2",
    "href": "listas/nivelamento/lista-nivelamento.html#resposta-2",
    "title": "8  Lista Nivelamento",
    "section": "10.6 Resposta",
    "text": "10.6 Resposta\nO código não funciona por conta de uma diferença na escrita entre a variável atribuída na primeira linha e a variável que estamos tentando printar na segunda linha. Mais especificamente, há um typo da letra “i”.",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lista Nivelamento</span>"
    ]
  },
  {
    "objectID": "listas/nivelamento/lista-nivelamento.html#questão-4",
    "href": "listas/nivelamento/lista-nivelamento.html#questão-4",
    "title": "8  Lista Nivelamento",
    "section": "10.7 Questão 4",
    "text": "10.7 Questão 4\nCrie um objeto chamado meu_nome e salve nele o seu nome como um texto (lembre-se de usar aspas). Em seguida, use a função a paste() para criar uma frase que diga “Meu nome é [meu_nome]”, substituindo [meu_nome] pelo objeto recém criado.",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lista Nivelamento</span>"
    ]
  },
  {
    "objectID": "listas/nivelamento/lista-nivelamento.html#resposta-3",
    "href": "listas/nivelamento/lista-nivelamento.html#resposta-3",
    "title": "8  Lista Nivelamento",
    "section": "10.8 Resposta",
    "text": "10.8 Resposta\nA função paste permite combinar texto e o valor guardado em variáveis. Com isso, é possível printar elementos da seguinte maneira:\n\nmeu_nome &lt;- \"Felipe Marques Esteves Lamarca\"\nprint(paste(\"Meu nome é\", meu_nome))\n\n[1] \"Meu nome é Felipe Marques Esteves Lamarca\"",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lista Nivelamento</span>"
    ]
  },
  {
    "objectID": "listas/nivelamento/lista-nivelamento.html#questão-5",
    "href": "listas/nivelamento/lista-nivelamento.html#questão-5",
    "title": "8  Lista Nivelamento",
    "section": "11.1 Questão 5",
    "text": "11.1 Questão 5\nCarregue o arquivo governadores.csv, que contém informações de algumas pessoas fictícias, e salve seu conteúdo no objeto pessoas. Depois de carregado os dados, use a função head para pré-visualizar as primeiras linhas do data.frame. Use comentários para explicar como você descobriu a forma correta de carrergar o arquivo.",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lista Nivelamento</span>"
    ]
  },
  {
    "objectID": "listas/nivelamento/lista-nivelamento.html#resposta-4",
    "href": "listas/nivelamento/lista-nivelamento.html#resposta-4",
    "title": "8  Lista Nivelamento",
    "section": "11.2 Resposta",
    "text": "11.2 Resposta\nPrimeiro, importamos os dados usando a função read_csv2, encontrada na internet a partir de uma rápida busca: https://livro.curso-r.com/13-2-importa%C3%A7%C3%A3o.html. A função é útil no caso em que desejamos der arquivos .csv separados por “;”. É claro, poderíamos ter utilizando também a função read_delim. Depois é só aplicar a função head para observar as primeiras 5 linhas.\n\n# https://livro.curso-r.com/13-2-importa%C3%A7%C3%A3o.html\npessoas &lt;- read_csv2(\"datasets/governadores.csv\")\nhead(pessoas)\n\n# A tibble: 6 × 5\n  uf    candidatura                  partido  pct_gastos pct_votos\n  &lt;chr&gt; &lt;chr&gt;                        &lt;chr&gt;         &lt;dbl&gt;     &lt;dbl&gt;\n1 GO    GUSTAVO MENDANHA MELO        PATRIOTA      26.8      25.2 \n2 GO    RONALDO RAMOS CAIADO         UNIÃO         38.6      51.8 \n3 GO    WOLMIR THEREZIO AMADO        PT             4.02      6.98\n4 GO    VITOR HUGO DE ARAUJO ALMEIDA PL            28.9      14.8 \n5 MG    ALEXANDRE KALIL              PSD           33.8      35.1 \n6 MG    CARLOS ALBERTO DIAS VIANA    PL            15.6       7.23",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lista Nivelamento</span>"
    ]
  },
  {
    "objectID": "listas/nivelamento/lista-nivelamento.html#questão-6",
    "href": "listas/nivelamento/lista-nivelamento.html#questão-6",
    "title": "8  Lista Nivelamento",
    "section": "11.3 Questão 6",
    "text": "11.3 Questão 6\nÉ comum que dados de pesquisas de opinião, i.e. surveys, sejam armazenados em arquivos SPSS ou Stata, criados pelos softwares de mesmo nome. Neste exercício, sua tarefa será carregar um arquivo um Stata com dados da pesquisa do World Values Survey, principal fonte de dados sobre valores, crenças e comportamentos das populações de diferentes países. Os dados do Brasil estão no arquivo wvs.dta. Carregue o arquivo salve os resultados no objeto wvs_stata.\nLink para os dados: https://www.worldvaluessurvey.org/WVSDocumentationWV7.jsp",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lista Nivelamento</span>"
    ]
  },
  {
    "objectID": "listas/nivelamento/lista-nivelamento.html#resposta-5",
    "href": "listas/nivelamento/lista-nivelamento.html#resposta-5",
    "title": "8  Lista Nivelamento",
    "section": "11.4 Resposta",
    "text": "11.4 Resposta\nPara ler bancos de dados .dta, basta instalar o pacote haven e importar o arquivo usando a função abaixo:\n\nwvs_stata &lt;- haven::read_dta(\"datasets/WVS_Cross-National_Wave_7_stata_v6_0.dta\")",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lista Nivelamento</span>"
    ]
  },
  {
    "objectID": "listas/nivelamento/lista-nivelamento.html#questão-7",
    "href": "listas/nivelamento/lista-nivelamento.html#questão-7",
    "title": "8  Lista Nivelamento",
    "section": "11.5 Questão 7",
    "text": "11.5 Questão 7\nDiferentes órgãos públicos no Brasil disponibilizam seus microdados administrativos para acesso. Um deles é o INEP, que tem uma página dedicada para download de bases de dados com informações sobre os exames e pesquisas que conduz.\nNeste exercício, sua tarefa será obter e carregar no R alguns microdados do Censo da Educação Superior de 2022, levantamento anual do INEP que coleta informações sobre as instituições de ensino superior, cursos de graduação e estudantes no Brasil todo. Os dados que deverão ser baixados são os referentes ao ano de 2022, que poderão ser baixados da página do INEP. Os dados estão em formato compactado (zipado) e deverão ser descompactados no diretório local do R antes de serem carregados. Uma vez descompactados, carregue o arquivo MICRODADOS_ED_SUP_IES_2022.CSV e salve o resultado no objeto alunos. Reporte o código e o seguinte:\n\n\nO nome das colunas do data.frame alunos:\n\n\nO número de linhas e de colunas do data.frame alunos:\n\n\nLink para os dados: https://www.gov.br/inep/pt-br/acesso-a-informacao/dados-abertos/microdados/censo-da-educacao-superior",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lista Nivelamento</span>"
    ]
  },
  {
    "objectID": "listas/nivelamento/lista-nivelamento.html#resposta-6",
    "href": "listas/nivelamento/lista-nivelamento.html#resposta-6",
    "title": "8  Lista Nivelamento",
    "section": "11.6 Resposta",
    "text": "11.6 Resposta\nNeste caso usamos a função read_delim para fazer a leitura dos dados. Além disso, foi importante especificar o encoding do dataset. Os nomes das colunas podem ser obtidos pela função colnames, enquanto número de linhas e colunas podem ser obtidos usando as funções nrow e ncol, respectivamente.\n\nalunos &lt;- read_delim(\"datasets/MICRODADOS_ED_SUP_IES_2022.CSV\", \n                     delim=\";\", \n                     locale=locale(encoding = \"latin1\"))\n\n\n# resposta da letra a)\nprint(colnames(alunos))\n\n [1] \"NU_ANO_CENSO\"                   \"NO_REGIAO_IES\"                 \n [3] \"CO_REGIAO_IES\"                  \"NO_UF_IES\"                     \n [5] \"SG_UF_IES\"                      \"CO_UF_IES\"                     \n [7] \"NO_MUNICIPIO_IES\"               \"CO_MUNICIPIO_IES\"              \n [9] \"IN_CAPITAL_IES\"                 \"NO_MESORREGIAO_IES\"            \n[11] \"CO_MESORREGIAO_IES\"             \"NO_MICRORREGIAO_IES\"           \n[13] \"CO_MICRORREGIAO_IES\"            \"TP_ORGANIZACAO_ACADEMICA\"      \n[15] \"TP_CATEGORIA_ADMINISTRATIVA\"    \"NO_MANTENEDORA\"                \n[17] \"CO_MANTENEDORA\"                 \"CO_IES\"                        \n[19] \"NO_IES\"                         \"SG_IES\"                        \n[21] \"DS_ENDERECO_IES\"                \"DS_NUMERO_ENDERECO_IES\"        \n[23] \"DS_COMPLEMENTO_ENDERECO_IES\"    \"NO_BAIRRO_IES\"                 \n[25] \"NU_CEP_IES\"                     \"QT_TEC_TOTAL\"                  \n[27] \"QT_TEC_FUNDAMENTAL_INCOMP_FEM\"  \"QT_TEC_FUNDAMENTAL_INCOMP_MASC\"\n[29] \"QT_TEC_FUNDAMENTAL_COMP_FEM\"    \"QT_TEC_FUNDAMENTAL_COMP_MASC\"  \n[31] \"QT_TEC_MEDIO_FEM\"               \"QT_TEC_MEDIO_MASC\"             \n[33] \"QT_TEC_SUPERIOR_FEM\"            \"QT_TEC_SUPERIOR_MASC\"          \n[35] \"QT_TEC_ESPECIALIZACAO_FEM\"      \"QT_TEC_ESPECIALIZACAO_MASC\"    \n[37] \"QT_TEC_MESTRADO_FEM\"            \"QT_TEC_MESTRADO_MASC\"          \n[39] \"QT_TEC_DOUTORADO_FEM\"           \"QT_TEC_DOUTORADO_MASC\"         \n[41] \"IN_ACESSO_PORTAL_CAPES\"         \"IN_ACESSO_OUTRAS_BASES\"        \n[43] \"IN_ASSINA_OUTRA_BASE\"           \"IN_REPOSITORIO_INSTITUCIONAL\"  \n[45] \"IN_BUSCA_INTEGRADA\"             \"IN_SERVICO_INTERNET\"           \n[47] \"IN_PARTICIPA_REDE_SOCIAL\"       \"IN_CATALOGO_ONLINE\"            \n[49] \"QT_PERIODICO_ELETRONICO\"        \"QT_LIVRO_ELETRONICO\"           \n[51] \"QT_DOC_TOTAL\"                   \"QT_DOC_EXE\"                    \n[53] \"QT_DOC_EX_FEMI\"                 \"QT_DOC_EX_MASC\"                \n[55] \"QT_DOC_EX_SEM_GRAD\"             \"QT_DOC_EX_GRAD\"                \n[57] \"QT_DOC_EX_ESP\"                  \"QT_DOC_EX_MEST\"                \n[59] \"QT_DOC_EX_DOUT\"                 \"QT_DOC_EX_INT\"                 \n[61] \"QT_DOC_EX_INT_DE\"               \"QT_DOC_EX_INT_SEM_DE\"          \n[63] \"QT_DOC_EX_PARC\"                 \"QT_DOC_EX_HOR\"                 \n[65] \"QT_DOC_EX_0_29\"                 \"QT_DOC_EX_30_34\"               \n[67] \"QT_DOC_EX_35_39\"                \"QT_DOC_EX_40_44\"               \n[69] \"QT_DOC_EX_45_49\"                \"QT_DOC_EX_50_54\"               \n[71] \"QT_DOC_EX_55_59\"                \"QT_DOC_EX_60_MAIS\"             \n[73] \"QT_DOC_EX_BRANCA\"               \"QT_DOC_EX_PRETA\"               \n[75] \"QT_DOC_EX_PARDA\"                \"QT_DOC_EX_AMARELA\"             \n[77] \"QT_DOC_EX_INDIGENA\"             \"QT_DOC_EX_COR_ND\"              \n[79] \"QT_DOC_EX_BRA\"                  \"QT_DOC_EX_EST\"                 \n[81] \"QT_DOC_EX_COM_DEFICIENCIA\"     \n\n# resposta da letra b)\nprint(paste(\"O número de linhas é\", nrow(alunos)))\n\n[1] \"O número de linhas é 2595\"\n\nprint(paste(\"O número de colunas é\", ncol(alunos)))\n\n[1] \"O número de colunas é 81\"",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lista Nivelamento</span>"
    ]
  },
  {
    "objectID": "listas/nivelamento/lista-nivelamento.html#questão-8",
    "href": "listas/nivelamento/lista-nivelamento.html#questão-8",
    "title": "8  Lista Nivelamento",
    "section": "12.1 Questão 8",
    "text": "12.1 Questão 8\nNo Google Classroom (Aula 2 Mural - https://classroom.google.com/c/NzQ4Mzk4MTMyMDU3), há um arquivo chamado base_municipios_brasileiros.xlsx com informações dos municípios brasileiros. Este banco possui mais de 400 variáveis. Crie um novo data.frame, selecionando apenas variáveis referentes a identificação do município, ano, população e pib. Descreva o processo e justifique suas escolhas.",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lista Nivelamento</span>"
    ]
  },
  {
    "objectID": "listas/nivelamento/lista-nivelamento.html#resposta-7",
    "href": "listas/nivelamento/lista-nivelamento.html#resposta-7",
    "title": "8  Lista Nivelamento",
    "section": "12.2 Resposta",
    "text": "12.2 Resposta\nInfelizmente o arquivo .xlsx continha algum problema de formato, o que impediu que eu conseguisse abri-lo mesmo localmente no Excel. Por isso, utilizou-se o arquivo .csv. A seleção das colunas foi feita utilizando a função select.\n\nbase_municipios_brasileiros &lt;- read_csv(\"datasets/base_municipios_brasileiros.csv\")\n\nbase_municipios_brasileiros &lt;- base_municipios_brasileiros %&gt;%\n                                  select(\"id_municipio\", \"nome_municipio\", \"ano\", \"populacao\", \"pib\")",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lista Nivelamento</span>"
    ]
  },
  {
    "objectID": "listas/nivelamento/lista-nivelamento.html#questão-9",
    "href": "listas/nivelamento/lista-nivelamento.html#questão-9",
    "title": "8  Lista Nivelamento",
    "section": "12.3 Questão 9",
    "text": "12.3 Questão 9\nUtilize o banco de dados do exercício anterior para criar um novo objeto/data.frame chamado populacao. Faça o filtro para selecionar somente informações de 2022, criando um novo data.frame. E, a partir dele, crie um novo objeto chamado maiores_populacoes contendo apenas os 50 municípios mais populados do país, ordenados de forma decrescente. Use comentários para indicar qual é o 50º município mais populoso do país.",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lista Nivelamento</span>"
    ]
  },
  {
    "objectID": "listas/nivelamento/lista-nivelamento.html#resposta-8",
    "href": "listas/nivelamento/lista-nivelamento.html#resposta-8",
    "title": "8  Lista Nivelamento",
    "section": "12.4 Resposta",
    "text": "12.4 Resposta\nPrimeiro, filtramos apenas a população de 2022 usando a função filter. Depois, bastou ordenar os municípios em ordem decrescente compondo as funções arrange e desc, e tomar as 50 primeiras linhas. Depois utilizamos a função tail para mostrar a última linha – isto é, a linha correspondente ao 50º município mais populoso.\n\npopulacao &lt;-base_municipios_brasileiros %&gt;%\n              filter(ano == 2022)\n\nmaiores_populacoes &lt;- populacao %&gt;%\n                        arrange(by=desc(populacao)) %&gt;%\n                        head(50)\n\nprint(tail(maiores_populacoes, 1))\n\n# A tibble: 1 × 5\n  id_municipio nome_municipio   ano populacao   pib\n         &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1      1100205 Porto Velho     2022    461748    NA",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lista Nivelamento</span>"
    ]
  },
  {
    "objectID": "listas/nivelamento/lista-nivelamento.html#questão-10",
    "href": "listas/nivelamento/lista-nivelamento.html#questão-10",
    "title": "8  Lista Nivelamento",
    "section": "12.5 Questão 10",
    "text": "12.5 Questão 10\nUsando o objeto populacao, criado no exercício anterior, adicione a ele uma nova variável chamada pequeno_porte. Nesta, constará os valores: Pequeno porte, para municípios com menos de 50 mil habitantes, e Outros para os demais municípios.",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lista Nivelamento</span>"
    ]
  },
  {
    "objectID": "listas/nivelamento/lista-nivelamento.html#resposta-9",
    "href": "listas/nivelamento/lista-nivelamento.html#resposta-9",
    "title": "8  Lista Nivelamento",
    "section": "12.6 Resposta",
    "text": "12.6 Resposta\nCombinando as funções mutate e case_when, conseguimos criar a coluna pequeno_porte.\n\npopulacao &lt;- populacao %&gt;%\n  mutate(pequeno_porte = case_when(\n    populacao &lt; 50000 ~ \"Pequeno porte\",\n    TRUE ~ \"Outros\"\n  )\n)",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lista Nivelamento</span>"
    ]
  },
  {
    "objectID": "listas/nivelamento/lista-nivelamento.html#questão-11",
    "href": "listas/nivelamento/lista-nivelamento.html#questão-11",
    "title": "8  Lista Nivelamento",
    "section": "12.7 Questão 11",
    "text": "12.7 Questão 11\nCarregue a base de dados banco_violencia.xlsx. A partir da função left_join, crie uma nova base cruzando essa base com o objeto populacao, criado anteriormente. Descreva quais as variáveis que são utilizadas para o cruzamento e por quê.",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lista Nivelamento</span>"
    ]
  },
  {
    "objectID": "listas/nivelamento/lista-nivelamento.html#resposta-10",
    "href": "listas/nivelamento/lista-nivelamento.html#resposta-10",
    "title": "8  Lista Nivelamento",
    "section": "12.8 Resposta",
    "text": "12.8 Resposta\nPodemos facilmente cruzar os dois bancos de dados utilizando a coluna id_municipio, presente nos dois datasets. Por se tratar de uma coluna padronizada, a variável de id é muito útil para realizar conexões entre os bancos de dados.\n\nbanco_violencia &lt;- read_excel(\"datasets/banco_violencia.xlsx\")\nbanco_violencia$id_municipio &lt;- as.double(banco_violencia$id_municipio) \npopulacao_violencia &lt;- left_join(x=populacao, y=banco_violencia, by=c(\"id_municipio\"))",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lista Nivelamento</span>"
    ]
  },
  {
    "objectID": "listas/nivelamento/lista-nivelamento.html#questão-12",
    "href": "listas/nivelamento/lista-nivelamento.html#questão-12",
    "title": "8  Lista Nivelamento",
    "section": "13.1 Questão 12",
    "text": "13.1 Questão 12\nUsando o banco de dados criado no exercício anterior, crie duas visualizações:\n\num histograma da população brasileira dividida por municípios\num gráfico com duas barras comparando o número de municípios de pequeno porte e municípios de outros tamanhos\n\nO que esses dois gráficos revelam sobre o caráter geral dos municípios brasileiros?\nDica: consulte o livro Usando o R para identificar os geoms mais adequados",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lista Nivelamento</span>"
    ]
  },
  {
    "objectID": "listas/nivelamento/lista-nivelamento.html#resposta-11",
    "href": "listas/nivelamento/lista-nivelamento.html#resposta-11",
    "title": "8  Lista Nivelamento",
    "section": "13.2 Resposta",
    "text": "13.2 Resposta\n\nhist(populacao_violencia$populacao.x)\n\n\n\n\n\n\n\n\n\npopulacao_violencia %&gt;%\n  group_by(pequeno_porte) %&gt;%\n  summarise(n_municipios = n()) %&gt;%\n  ggplot(aes(x = pequeno_porte, y = n_municipios)) +\n  geom_col()\n\n\n\n\n\n\n\n\nOs dois gráficos plotados nos mostram que a esmagadora maioria dos municípios brasileiros são de pequeno porte – isto é, possuem menos de 50 mil habitantes.",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lista Nivelamento</span>"
    ]
  },
  {
    "objectID": "listas/nivelamento/lista-nivelamento.html#questão-13",
    "href": "listas/nivelamento/lista-nivelamento.html#questão-13",
    "title": "8  Lista Nivelamento",
    "section": "13.3 Questão 13",
    "text": "13.3 Questão 13\nNovamente usando o banco violencia, crie uma visualização que mostre a evolução do número absoluto de mortes por intervenção policial no município do Rio de Janeiro ao longo dos anos.\nDica 1: para facilitar seu trabalho, você pode criar um novo banco contendo apenas as informações do município de interesse para o gráfico usando a função de manipulação filter()\nDica 2: verifique se a classe das variáveis utilizadas é numérica. Caso não seja, faça a conversão usando a função mutate()",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lista Nivelamento</span>"
    ]
  },
  {
    "objectID": "listas/nivelamento/lista-nivelamento.html#resposta-12",
    "href": "listas/nivelamento/lista-nivelamento.html#resposta-12",
    "title": "8  Lista Nivelamento",
    "section": "13.4 Resposta",
    "text": "13.4 Resposta\nApós filtrar os dados para o município do Rio de Janeiro, podemos usar a coluna quantidade_mortes_intervencao_policial (transformada em uma variável numérica através da função mutate) para plotar um gráfico de série temporal.\n\nbanco_violencia %&gt;%\n  filter(id_municipio_nome == 'Rio de Janeiro') %&gt;%\n  mutate(quantidade_mortes_intervencao_policial = as.numeric(quantidade_mortes_intervencao_policial)) %&gt;%\n  ggplot(aes(x = ano, y = quantidade_mortes_intervencao_policial, group=1)) +\n  geom_line() +\n  geom_point()",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lista Nivelamento</span>"
    ]
  },
  {
    "objectID": "listas/nivelamento/lista-nivelamento.html#questão-14",
    "href": "listas/nivelamento/lista-nivelamento.html#questão-14",
    "title": "8  Lista Nivelamento",
    "section": "13.5 Questão 14",
    "text": "13.5 Questão 14\nNovamente usando o banco do exercício 11, crie um novo banco contabilizando a população por estado. Em seguida, crie um mapa que permita visualizar o tamalho da população de cada estado.\nDica 1: para a manipulação, use as funções group_by() e sumarise(), descritas no capítulo 3 do livro Usando R.\nDica 2: para a elaboração do mapa, será necessário utilizar os pacotes geobr e sf",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lista Nivelamento</span>"
    ]
  },
  {
    "objectID": "listas/nivelamento/lista-nivelamento.html#resposta-13",
    "href": "listas/nivelamento/lista-nivelamento.html#resposta-13",
    "title": "8  Lista Nivelamento",
    "section": "13.6 Resposta",
    "text": "13.6 Resposta\nComeçamos lendo os dados de geometria dos estados brasileiros utilizando o geobr. Daí, basta agrupar os dados por unidade da federação, somando o total da população, e cruzar as bases pela sigla do estado. O gráfico é plotado utilizando a geometria sf.\n\nlibrary(geobr)\nlibrary(sf)\n\ncoordenadas_estados &lt;- read_state()\n\npopulacao_violencia %&gt;%\n  group_by(sigla_uf) %&gt;%\n  summarise(total_populacao = sum(populacao.x)) %&gt;%\n  left_join(coordenadas_estados, by = c(\"sigla_uf\" = \"abbrev_state\")) %&gt;%\n  st_as_sf() %&gt;%\n  ggplot(aes(fill = total_populacao)) + \n  geom_sf()",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lista Nivelamento</span>"
    ]
  },
  {
    "objectID": "listas/nivelamento/lista-nivelamento.html#questão-15",
    "href": "listas/nivelamento/lista-nivelamento.html#questão-15",
    "title": "8  Lista Nivelamento",
    "section": "13.7 Questão 15",
    "text": "13.7 Questão 15\nSelecione um dos gráficos elaborados nos exercícios anteriores e faça customizações nele. Você pode mudar cores, temas, acrescentar títulos, mudar nomes de variáveis… deixe sua criatividade fluir, mas sempre priorizando a clareza e a melhor forma de comunicar as informações contidas no gráfico.",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lista Nivelamento</span>"
    ]
  },
  {
    "objectID": "listas/nivelamento/lista-nivelamento.html#resposta-14",
    "href": "listas/nivelamento/lista-nivelamento.html#resposta-14",
    "title": "8  Lista Nivelamento",
    "section": "13.8 Resposta",
    "text": "13.8 Resposta\nFaço algumas modificações ao gráfico de linha anterior. Em especial, ajusto o eixo \\(y\\) para começar no 0, o que ajusta a analisar de maneira mais adequada a variação no número de mortes ao longo do tempo. Além disso, incluo também título, subtítulo e caption, além de alguns ajustes mais residuais no tamanho e estilo da fonte. Alterei também o titulo dos eixos para facilitar a compreensão das variáveis.\n\nbanco_violencia %&gt;%\n  filter(id_municipio_nome == 'Rio de Janeiro') %&gt;%\n  mutate(quantidade_mortes_intervencao_policial = as.numeric(quantidade_mortes_intervencao_policial)) %&gt;%\n  ggplot(aes(x = ano, y = quantidade_mortes_intervencao_policial, group = 1)) +\n  geom_line(color = \"steelblue\") +\n  geom_point(color = \"steelblue\") +\n  scale_y_continuous(expand = c(0, 0), limits = c(0, 800)) +\n  labs(\n    title = \"Mortes por Intervenção Policial no Rio de Janeiro por Ano\",\n    subtitle = \"Dados do Instituto de Segurança Pública\",\n    caption = \"Produzido por Felipe Lamarca\",\n    x = \"Ano\",\n    y = \"Quantidade de Mortes\"\n  ) +\n  theme_bw() +\n  theme(\n    plot.title = element_text(size = 14, face = \"bold\"),\n    axis.title.x = element_text(size = 12),\n    axis.title.y = element_text(size = 12),\n    axis.text.x = element_text(size = 10),\n    axis.text.y = element_text(size = 10)\n  )",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Lista Nivelamento</span>"
    ]
  },
  {
    "objectID": "listas/lista-1/lista1.html",
    "href": "listas/lista-1/lista1.html",
    "title": "9  Lista 1",
    "section": "",
    "text": "10 Materiais de apoio\nEsta lista de exercícios é baseada no conteúdo dos seguintes textos:",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Lista 1</span>"
    ]
  },
  {
    "objectID": "listas/lista-1/lista1.html#delimitação-disciplinar",
    "href": "listas/lista-1/lista1.html#delimitação-disciplinar",
    "title": "9  Lista 1",
    "section": "11.1 1) Delimitação disciplinar",
    "text": "11.1 1) Delimitação disciplinar\nDentro da sua disciplina, aponte uma área de pesquisa na qual você tem interesse, como Sociologia da Cultura, Estudos Legislativos, entre outras.",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Lista 1</span>"
    ]
  },
  {
    "objectID": "listas/lista-1/lista1.html#resposta",
    "href": "listas/lista-1/lista1.html#resposta",
    "title": "9  Lista 1",
    "section": "11.2 Resposta",
    "text": "11.2 Resposta\nDinheiro na política e machine learning.",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Lista 1</span>"
    ]
  },
  {
    "objectID": "listas/lista-1/lista1.html#problema-de-pesquisa",
    "href": "listas/lista-1/lista1.html#problema-de-pesquisa",
    "title": "9  Lista 1",
    "section": "11.3 2) Problema de pesquisa",
    "text": "11.3 2) Problema de pesquisa\n(Máx: 5 linhas). Defina um problema de pesquisa – pode estar relacionado a tese, dissertação ou outro projeto que você esteja desenvolvendo.\n\n11.3.1 Instruções:\n\nMantenha-se dentro do sub-campo disciplinar definido. Lembre-se de que um problema empírico em Ciências Sociais geralmente relaciona dois fatores ou fenômenos de forma causal. Por exemplo, é possível relacionar o efeito da desigualdade de renda na participação política; ou o efeito da discriminação racial sobre o mercado de trabalho; ou o crescimento econômico sobre a democracia.\nFormule seu problema de pesquisa na forma de uma pergunta. Exemplos: “Qual é o efeito da desigualdade de renda na participação política?”; “Por que populações menos escolarizadas têm menor acesso a benefícios sociais?”. É possível contextualizar o problema, indicando a relevância do tema e ou uma lacuna na literatura.",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Lista 1</span>"
    ]
  },
  {
    "objectID": "listas/lista-1/lista1.html#resposta-1",
    "href": "listas/lista-1/lista1.html#resposta-1",
    "title": "9  Lista 1",
    "section": "11.4 Resposta",
    "text": "11.4 Resposta\nNo meu pré-projeto de dissertação, a pergunta que propus responder foi: “Qual é o efeito do dinheiro no sucesso eleitoral em eleições legislativas municipais no Brasil?”; ou, de maneira mais genérica “Por que alguns candidatos são eleitos em detrimento de outros?”. A relação entre dinheiro e eleições é amplamente explorada na literatura (Mancuso 2015), mas poucos trabalhos investiram em análises inferenciais ou preditivas sobre essa relação no nível municipal (Sampaio and Filho 2019).",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Lista 1</span>"
    ]
  },
  {
    "objectID": "listas/lista-1/lista1.html#definição-de-conceitos",
    "href": "listas/lista-1/lista1.html#definição-de-conceitos",
    "title": "9  Lista 1",
    "section": "11.5 3) Definição de conceitos",
    "text": "11.5 3) Definição de conceitos\n(Máx: 5 linhas). Com um problema definido, transforme agora os fatores ou fenômenos do seu interesse em conceitos, isto é, em termos teóricos mais precisos e que sejam passíveis de mensuração.\n\n11.5.1 Instruções:\n\nPor exemplo, “pobreza” é um conceito amplo que pode ser definido como “a carência de elementos essenciais para a sobrevivência ou o bem-estar humano”, o que é algo ligeiramente mais concreto e passível de mensuração. Definições de conceitos podem ser retiradas da literatura prévia, mas é importante que você os descreva de forma compreensível.",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Lista 1</span>"
    ]
  },
  {
    "objectID": "listas/lista-1/lista1.html#resposta-2",
    "href": "listas/lista-1/lista1.html#resposta-2",
    "title": "9  Lista 1",
    "section": "11.6 Resposta",
    "text": "11.6 Resposta\nEm certa medida, no caso particular deste problema de pesquisa, os conceitos e as variáveis operacionalizadas se confundem. “Sucesso eleitoral” pode representar, efetivamente, se o candidato conseguiu obter o determinado cargo legislativo em disputa1; “dinheiro” representa, do ponto de vista financeiro, quanto foi investido naquela disputa eleitoral pelo candidato.",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Lista 1</span>"
    ]
  },
  {
    "objectID": "listas/lista-1/lista1.html#operacionalização-de-variáveis",
    "href": "listas/lista-1/lista1.html#operacionalização-de-variáveis",
    "title": "9  Lista 1",
    "section": "11.7 4) Operacionalização de variáveis",
    "text": "11.7 4) Operacionalização de variáveis\n(Máx: 5 linhas). Com seus conceitos definidos, nosso próximo passo é especificar como você os medirá. Essa etapa, chamada de operacionalização de variáveis, deve ser feita de forma a garantir que as medidas ou indicadores utilizados tenham aderência aos conceitos mobilizados, isto é, que realmente capturem aspectos do conceitos que você quer estudar. Este é o momento para pensar no recorte do objeto de forma a delimitar quais seriam as observações (i.e., quais seriam as linhas no seu banco de dados) e as unidades.",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Lista 1</span>"
    ]
  },
  {
    "objectID": "listas/lista-1/lista1.html#resposta-3",
    "href": "listas/lista-1/lista1.html#resposta-3",
    "title": "9  Lista 1",
    "section": "11.8 Resposta",
    "text": "11.8 Resposta\nA variável de sucesso eleitoral é operacionalizada a partir do resultado nas urnas – ou (1) uma variável contínua que guarda o número de votos de um candidato nas urnas, ou (2) uma variável binária que indica se o candidato foi eleito ou não2. Opto pela segunda abordagem. Além disso, “dinheiro” é operacionalizado através das despesas de campanha do candidato, disponibilizadas pelo TSE. Também seria razoável utilizar a receita, sob o risco de superestimar o efeito do dinheiro.",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Lista 1</span>"
    ]
  },
  {
    "objectID": "listas/lista-1/lista1.html#variáveis",
    "href": "listas/lista-1/lista1.html#variáveis",
    "title": "9  Lista 1",
    "section": "11.9 5) Variáveis",
    "text": "11.9 5) Variáveis\n(Máx: 2 linhas). Indique qual das duas variáveis definidas anteriormente é a sua variável dependente, aquela que será explicada, e qual delas é a sua variável independente, a que supostamente explica a primeira.",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Lista 1</span>"
    ]
  },
  {
    "objectID": "listas/lista-1/lista1.html#resposta-4",
    "href": "listas/lista-1/lista1.html#resposta-4",
    "title": "9  Lista 1",
    "section": "11.10 Resposta",
    "text": "11.10 Resposta\nA despesa de campanha é a variável independente, e o sucesso eleitoral é a variável dependente.",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Lista 1</span>"
    ]
  },
  {
    "objectID": "listas/lista-1/lista1.html#teoria-causal",
    "href": "listas/lista-1/lista1.html#teoria-causal",
    "title": "9  Lista 1",
    "section": "12.1 6) Teoria causal",
    "text": "12.1 6) Teoria causal\n(Máx: 7 linhas). Com o problema de pesquisa definido e variáveis operacionalizadas, elabore de forma concisa uma teoria que descreva como a sua variável independente causa a sua dependente.\n\n12.1.1 Instruções:\n\nAo formular uma teoria, pense em responder à pergunta “por que Y ocorre?” usando X como parte da sua explicação. Por exemplo: o que estaria por detrás das menores taxas de comparecimento eleitoral dos menos escolarizados? Será que essas pessoas, por terem menos acesso a determinadas informações, teriam também menos interesse em política e, por consequência, menor disposição de ir votar? Outro exemplo: pessoas mais pobres comparecem menos às urnas por conta de maiores dificuldades de transporte (em geral, moram em lugares de mais difícil acesso e dispõem de menos recursos para se locomoverem até os locais de votação).",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Lista 1</span>"
    ]
  },
  {
    "objectID": "listas/lista-1/lista1.html#resposta-5",
    "href": "listas/lista-1/lista1.html#resposta-5",
    "title": "9  Lista 1",
    "section": "12.2 Resposta",
    "text": "12.2 Resposta\nAs eleições brasileiras são caracterizadas por serem altamente competitivas em função, dentre outros aspectos, da alta oferta de candidatos. Portanto, altas despesas de campanha permitem que o candidato seja capaz de alcançar maior visibilidade e se diferenciar ao saturar o eleitor com diferentes estratégias de publicidade, financeiramente custosas, como material de campanha, presença online etc. Dito de outra maneira: ao conseguir reunir mais dinheiro para fazer campanha, o candidato tende a aumentar a viabilidade de sua candidatura na medida em que se torna capaz de alcançar o eleitor, ampliando a sua visibilidade e fortalecendo a estrutura da campanha.",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Lista 1</span>"
    ]
  },
  {
    "objectID": "listas/lista-1/lista1.html#hipótese-principal",
    "href": "listas/lista-1/lista1.html#hipótese-principal",
    "title": "9  Lista 1",
    "section": "12.3 7) Hipótese principal",
    "text": "12.3 7) Hipótese principal\n(Máx: 3 linhas). Hipóteses especificam o sentido e forma de uma relação entre X e Y (positiva, negativa, nula, linear, quadrática, etc.). Por exemplo, “uma menor taxa de escolaridade diminui participação política”, “quanto maior a desigualdade fundiária, maior a taxa de migração de jovens do campo para a cidade”. Partindo dessa definição, a) formule uma hipótese principal para a sua teoria, b) e descreva como seria um gráfico de dispersão (scatterplot) que ilustrasse a relação entre X e Y de acordo com a sua hipótese.",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Lista 1</span>"
    ]
  },
  {
    "objectID": "listas/lista-1/lista1.html#resposta-6",
    "href": "listas/lista-1/lista1.html#resposta-6",
    "title": "9  Lista 1",
    "section": "12.4 Resposta",
    "text": "12.4 Resposta\n\nQuanto maior o gasto em campanha despendidos pelos candidatos, maior é a probabilidade de eleição do candidato.\nEm uma relação não-linear, quanto mais “longe” no eixo \\(X\\) (isto é, quanto maior a despesa), mais “longe” no eixo \\(Y\\) (isto é, mais a probabilidade de eleição se aproxima de \\(1\\)).\n\n\nlibrary(tidyverse)\n\nset.seed(123)\n\n# candidatos simulados\nn &lt;- 1000\n\n# simulacao das despesas de campanha (distribuicao exponencial com media 30 mil)\n# documentacao da funcao \n# https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/Exponential\ndespesas &lt;- rexp(n, rate = 1/30)\n\n# efeito da despesa\nalpha &lt;- -2\nefeito_despesa &lt;- 0.05\n\n# calcula prob de eleicao\nprob_eleito &lt;- 1 / (1 + exp(-(alpha + efeito_despesa * despesas)))\n\n# dataframe\ndados &lt;- data.frame(despesas, prob_eleito)\n\n# plot da relacao logistica\nggplot(dados, aes(x = despesas, y = prob_eleito)) +\n  geom_jitter(height = 0.05, alpha = 0.3) +\n  stat_smooth(method = \"glm\", \n              method.args = list(family = \"binomial\"), \n              se = FALSE, \n              color = \"darkgreen\") +\n  labs(\n    title = \"Probabilidade de eleição dada a despesa de campanha\",\n    x = \"Despesas de campanha (R$ mil)\",\n    y = \"Probabilidade de eleição\"\n  ) +\n  theme_bw()",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Lista 1</span>"
    ]
  },
  {
    "objectID": "listas/lista-1/lista1.html#implicações-testáveis-mecanismo",
    "href": "listas/lista-1/lista1.html#implicações-testáveis-mecanismo",
    "title": "9  Lista 1",
    "section": "12.5 8) Implicações testáveis (mecanismo)",
    "text": "12.5 8) Implicações testáveis (mecanismo)\n(Máx: 5 linhas). Se a sua teoria causal estiver certa, dela também devem decorrer outras implicações observáveis. Você conseguiria pensar em fenômenos empíricos secundários (associados ao principal, definido na pergunta de pesquisa) que decorreriam da sua explicação? Defina ao menos uma hipótese secundária testável que decorra da sua teoria causal.\n\n12.5.1 Instruções:\n\nUm exemplo: na teoria que relaciona menor escolaridade a menor participação política, é possível hipotetizar que pessoas menos escolarizadas devem acompanhar menos o noticiário político (este seria o mecanismo causal); nesse sentido, uma implicação empírica secundária consistente com a teoria é a de que pessoas menos escolarizadas devem consumir menos notícias políticas, o que poderia ser mensurado por meio de uma pesquisa de opinião perguntando sobre o consumo de notícias políticas (“Quantos dias na semana você costuma assistir ao noticiário na TV?”).",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Lista 1</span>"
    ]
  },
  {
    "objectID": "listas/lista-1/lista1.html#resposta-7",
    "href": "listas/lista-1/lista1.html#resposta-7",
    "title": "9  Lista 1",
    "section": "12.6 Resposta",
    "text": "12.6 Resposta\nAssumindo que a teoria causal esteja certa, poderíamos imaginar, por exemplo, que candidatos que gastam mais dinheiro também são mais conhecidos pelo eleitorado. Naturalmente, isso poderia ser testado empiricamente através de uma pesquisa de survey que estime a taxa de desconhecimento por parte dos eleitores a respeito dos candidatos na disputa, ou mesmo as intenções de voto.",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Lista 1</span>"
    ]
  },
  {
    "objectID": "listas/lista-1/lista1.html#desenho-de-pesquisa",
    "href": "listas/lista-1/lista1.html#desenho-de-pesquisa",
    "title": "9  Lista 1",
    "section": "13.1 9) Desenho de pesquisa",
    "text": "13.1 9) Desenho de pesquisa\n(Máx: 1 linha para cada item). Agora que você tem uma teoria, conceitos, variáveis definidas e hipóteses, preencha de forma sucinta os itens a seguir – eles te darão uma versão compacta e descritiva do seu desenho de pesquisa quantitativo em Ciências Sociais.\n\n\nPergunta de pesquisa:\n\nQual é o efeito do dinheiro no sucesso eleitoral em eleições legislativas municipais no Brasil?\n\nConceitos mobilizados:\n\nSucesso eleitoral e dinheiro.\n\nVariável dependente:\n\nSucesso eleitoral (eleito ou não).\n\nVariável independente:\n\nDinheiro (despesas de campanha).\n\nHipótese principal:\n\nQuanto maior a despesa, maior a chance do candidato se eleger.\n\nHipótese secundária (mecanismo):\n\nCandidatos que gastam mais também são mais conhecidos.\n\nImplicações testáveis do mecanismo:\n\nEm pesquisas de intenção de voto, candidatos que gastam mais possuem menores taxas de desconhecimento atreladas a eles.\n\n\n\n\n\nMancuso, Wagner Pralon. 2015. “Investimento Eleitoral no Brasil: Balanço da Literatura (2001-2012).” Revista de Sociologia e Política 23 (54): 155–83.\n\n\nSampaio, Daniel, and Dalson Britto Figueiredo Filho. 2019. “Como o dinheiro influencia as eleições municipais no Brasil: uma revisão sistemática.” Revista Brasileira de Informação Bibliográfica Em Ciências Sociais - BIB, no. 88: 1–25.\n\n\nSpeck, Bruno Wilhelm, and Wagner Pralon Mancuso. 2014. “A Study on the Impact of Campaign Finance, Political Capital and Gender on Electoral Performance.” Brazilian Political Science Review 8 (1): 34–57.",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Lista 1</span>"
    ]
  },
  {
    "objectID": "listas/lista-1/lista1.html#footnotes",
    "href": "listas/lista-1/lista1.html#footnotes",
    "title": "9  Lista 1",
    "section": "",
    "text": "Há outras maneiras de operacionalizar essa variável, como sugerido na questão 4.↩︎\nAinda há pelo menos mais uma variação dessa abordagem, que modela o sucesso eleitoral como uma variável binária em função do número de votos, pensando nas limitações impostas por eleições proporcionais de lista aberta, em que o candidato mais votado não é necessariamente o eleito. Speck and Mancuso (2014), por exemplo, definem sucesso eleitoral como uma variável binária, em que os \\(n\\) candidatos mais votados assumem valor \\(1\\), enquanto os demais assumem o valor \\(0\\), sendo \\(n\\) o número de cadeiras em disputa no estado. É uma diferença sutil que, empiricamente, não difere tanto da modelagem padrão (vi no meu TCC!).↩︎",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Lista 1</span>"
    ]
  },
  {
    "objectID": "listas/lista-2/lista2.html",
    "href": "listas/lista-2/lista2.html",
    "title": "10  Lista II",
    "section": "",
    "text": "10.1 1. Importação\nVamos começar importando os dados. Mais especificamente, primeiro vamos ler os arquivos com as teses e dissertações (divididas em 6 tabelas diferentes) para, depois, ler os dados dos programas (uma única tabela). Utilizando a função list.files, podemos nos valer do argumento pattern na função para acessar apenas os arquivos que respeitam alguma regra específica – no nosso caso, aqueles que começam com “capes”:\n# listando os arquivos\nlist.files(path=\"data\", patter=\"^capes\")\n\n[1] \"capes_1987-1992.csv\" \"capes_1993-1998.csv\" \"capes_1999-2004.csv\"\n[4] \"capes_2005-2010.csv\" \"capes_2011-2016.csv\" \"capes_2017-2022.csv\"\npaths_list &lt;- list.files(path=\"data\", pattern=\"^capes\")\ndatasets &lt;- list()\n\nfor (i in 1:length(paths_list)) {\n  datasets[[i]] &lt;- read.csv(file = file.path(\"data\", paths_list[i]))\n}\n\nteses_dissertacoes &lt;- datasets %&gt;%\n  bind_rows()\nAgora, vamos ler o arquivo programas.csv:\nprogramas &lt;- read.csv(\"data/programas.csv\")\nhead(programas)\n\n    CD_PROGRAMA UF CONCEITO\n1 10001018041P7 RO        3\n2 10001018009P6 RO        4\n3 10001018016P2 RO        4\n4 10001018039P2 RO        4\n5 10001018017P9 RO        3\n6 10001018006P7 RO        3\nFelizmente, as bases compartilham a coluna CD_PROGRAMA (ou codigo_programa), identificando o código do programa de pós-graduação. Isso facilita bastante a junção das bases.\nteses_dissertacoes &lt;- left_join(\n  x=teses_dissertacoes, \n  y=programas, \n  join_by(codigo_programa == CD_PROGRAMA)\n  )\nSabemos que alguns trabalhos na base foram defendidos em programas de pós-graduação posteriormente descontinuados ou transformados, o que significa que suas notas de avaliação têm missing.\ncolSums(is.na(teses_dissertacoes)) / nrow(teses_dissertacoes)\n\n              ano   codigo_programa         sigla_ies          nome_ies \n       0.00000000        0.00000000        0.00000000        0.00000000 \n    nome_programa       grande_area area_conhecimento    area_avaliacao \n       0.00000000        0.00000000        0.00000000        0.00000000 \n            autor            titulo             nivel    palavras_chave \n       0.00000000        0.00000000        0.00000000        0.00000000 \n           resumo                UF          CONCEITO \n       0.00000000        0.06421002        0.06421002\nTrata-se de aproximadamente ~6,5% da base. Não trataremos os casos por ora, já que, no fim das contas, as teses e dissertações foram defendidas e as temáticas abordadas podem ser importantes para algumas agendas de pesquisa. De fato, caso eventualmente precisemos fazer gráficos que utilizem as colunas UF ou CONCEITO, essas entradas serão “dropadas”.",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Lista II</span>"
    ]
  },
  {
    "objectID": "listas/lista-2/lista2.html#seleção-de-palavras-chave",
    "href": "listas/lista-2/lista2.html#seleção-de-palavras-chave",
    "title": "10  Lista II",
    "section": "10.2 2. Seleção de palavras-chave",
    "text": "10.2 2. Seleção de palavras-chave\nVamos filtrar os resumos de todas as teses e dissertações na base, garantindo que mantenhamos apenas as dissertações cujo resumo apresenta, pelo menos uma vez, alguma das palavras ou termos a seguir: “proposições legislativas”, “text as data” e “polarização”. O primeiro passo é padronizar a coluna dos resumos para que todos estejam em lower case.\n\nteses_dissertacoes$resumo &lt;- str_to_lower(teses_dissertacoes$resumo)\n\nAgora, vamos filtrar as palavras especificadas:\n\nteses_dissertacoes$proposicoes_legislativas &lt;- str_detect(\n  teses_dissertacoes$resumo,\n  \"proposições legislativas\"\n  )\n\nteses_dissertacoes$text_as_data &lt;- str_detect(\n  teses_dissertacoes$resumo, \n  \"text as data\"\n  )\n\nteses_dissertacoes$polarizacao &lt;- str_detect(\n  teses_dissertacoes$resumo, \n  \"polarização\"\n  )\n\nteses_dissertacoes_flt &lt;- teses_dissertacoes %&gt;%\n  filter(\n    text_as_data == TRUE | proposicoes_legislativas == TRUE | polarizacao == TRUE\n    )\n\ncat(\"A partir de agora vamos trabalhar com\", \n    nrow(teses_dissertacoes_flt), \n    \"teses/dissertações.\")\n\nA partir de agora vamos trabalhar com 187 teses/dissertações.",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Lista II</span>"
    ]
  },
  {
    "objectID": "listas/lista-2/lista2.html#evolução-ao-longo-do-tempo",
    "href": "listas/lista-2/lista2.html#evolução-ao-longo-do-tempo",
    "title": "10  Lista II",
    "section": "10.3 3. Evolução ao longo do tempo",
    "text": "10.3 3. Evolução ao longo do tempo\nA melhor maneira de visualizar dados ao longo do tempo é utilizando séries temporais. No nosso caso, observamos que o número de teses e dissertações defendidas (e, claro, que citam no resumo os termos de interesse) vem crescendo ao longo do tempo. Isso se justifica pelo crescente interesse da literatura pela polarização, termo que domina os demais termos com \\(141\\) ocorrências.\n\nteses_dissertacoes_flt %&gt;%\n  group_by(ano) %&gt;%\n  summarise(n = n()) %&gt;%\n  ggplot(aes(x = ano, y = n)) +\n  geom_line(color = \"steelblue\", linewidth = 1) +\n  geom_point(color = \"steelblue\", size = 2) +\n  labs(\n    title = \"Evolução anual de teses e dissertações\",\n    subtitle = \"Teses e dissertações que citam pelo menos uma das expressões 'polarização',\\n 'text as data' e 'proposições legislativas'\",\n    x = \"Ano\",\n    y = \"Número de Trabalhos\",\n    caption = \"Feito por Felipe Lamarca usando dados da CAPES\"\n  ) +\n  theme_bw(base_size = 12) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(size = 12),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    panel.grid.minor = element_blank()\n  ) +\n  scale_x_continuous(breaks = seq(min(teses_dissertacoes_flt$ano), \n                                  max(teses_dissertacoes_flt$ano), by = 1))",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Lista II</span>"
    ]
  },
  {
    "objectID": "listas/lista-2/lista2.html#diferenças-regionais",
    "href": "listas/lista-2/lista2.html#diferenças-regionais",
    "title": "10  Lista II",
    "section": "10.4 4. Diferenças regionais",
    "text": "10.4 4. Diferenças regionais\nAgora vamos criar gráficos por estado e região, e, para isso excluiremos as linhas em que UF é missing. Essa não seria necessariamente a única abordagem possível: poderíamos, por exemplo, intuir (ou, quando possível, extrair usando regex) a unidade federativa a partir do nome da universidade. Optaremos pela primeira abordagem, bem mais simples – mesmo porque o percentual de missing é, apesar de relevante, pequeno.\n\ncolSums(is.na(teses_dissertacoes_flt)) / nrow(teses_dissertacoes_flt)\n\n                     ano          codigo_programa                sigla_ies \n              0.00000000               0.00000000               0.00000000 \n                nome_ies            nome_programa              grande_area \n              0.00000000               0.00000000               0.00000000 \n       area_conhecimento           area_avaliacao                    autor \n              0.00000000               0.00000000               0.00000000 \n                  titulo                    nivel           palavras_chave \n              0.00000000               0.00000000               0.00000000 \n                  resumo                       UF                 CONCEITO \n              0.00000000               0.05882353               0.05882353 \nproposicoes_legislativas             text_as_data              polarizacao \n              0.00000000               0.00000000               0.00000000 \n\n\nPrimeiro, vamos ler as informações de geometria, necessários para a geração dos mapas:\n\ncoordenadas_estados &lt;- read_state()\ncoordenadas_regiao &lt;- read_region()\n\nAgora vamos gerar o gráfico por região, manipulando a coluna UF usando a função mutate para criar essa nova coluna:\n\n# frequencia por regiao\nteses_dissertacoes_flt %&gt;%\n  drop_na() %&gt;%\n  mutate(\n    regiao = case_when(\n      UF %in% c(\"PA\", \"RN\") ~ \"Norte\",\n      UF %in% c(\"BA\", \"CE\", \"MA\", \"PB\", \"PE\") ~ \"Nordeste\",\n      UF %in% c(\"DF\", \"GO\") ~ \"Centro Oeste\",\n      UF %in% c(\"MG\", \"RJ\", \"SP\") ~ \"Sudeste\",\n      UF %in% c(\"PR\", \"RS\", \"SC\") ~ \"Sul\",\n      TRUE ~ NA_character_\n    )\n  ) %&gt;%\n  group_by(regiao) %&gt;%\n  summarise(n = n()) %&gt;%\n  full_join(coordenadas_regiao, join_by(regiao == name_region)) %&gt;%\n  st_as_sf() %&gt;%\n  ggplot(aes(fill = n)) +\n  geom_sf(color = \"white\", size = 0.3) +\n  scale_fill_gradient(\n    name = \"Nº de Trabalhos\",\n    low = \"#deebf7\", high = \"#08519c\"\n  ) +\n  labs(\n    title = \"Distribuição de teses e dissertações por região\",\n    subtitle = \"Teses e dissertações que citam pelo menos uma das expressões 'polarização',\\n 'text as data' e 'proposições legislativas'\",\n    caption = \"Feito por Felipe Lamarca usando dados da CAPES\"\n  ) +\n  theme_void(base_size = 12) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(size = 12),\n    legend.position = \"right\",\n    panel.grid = element_blank()\n  )\n\n\n\n\n\n\n\n\nE, por fim, vamos gerar o mapa por UF:\n\n# frequencia por UF\nteses_dissertacoes_flt %&gt;%\n  drop_na() %&gt;%\n  group_by(UF) %&gt;%\n  summarise(\n    n = n()\n  ) %&gt;%\n  full_join(coordenadas_estados, join_by(UF == abbrev_state)) %&gt;%\n  st_as_sf() %&gt;%\n  ggplot(aes(fill = n)) +\n  geom_sf(color = \"white\", size = 0.3) +\n  scale_fill_gradient(\n    name = \"Nº de Trabalhos\",\n    low = \"#deebf7\", high = \"#08519c\"\n  ) +\n  labs(\n    title = \"Distribuição de teses e dissertações por estado\",\n    subtitle = \"Teses e dissertações que citam pelo menos uma das expressões 'polarização',\\n'text as data' e 'proposições legislativas'. \\nEstados em cinza não possuem teses e dissertações no tema.\",\n    caption = \"Feito por Felipe Lamarca usando dados da CAPES.\"\n  ) +\n  theme_void(base_size = 12) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    plot.subtitle = element_text(size = 12),\n    legend.position = \"right\",\n    panel.grid = element_blank()\n  )\n\n\n\n\n\n\n\n\nNão é nenhuma surpresa que o número de teses e dissertações defendidas se concentre nos estados do Sul e Sudeste – em São Paulo, em particular. De fato, esse resultado deve se repetir para uma série de outros temas e palavras-chave. Essa análise provavelmente é, por si só, enviesada pela potencial concentração de programas de pós-graduação no Sudeste, assim como pela longevidade dos programas na região.\nEssa é apenas uma hipótese que, pelo menos nessa ocasião, não será testada. Mas poderíamos, por exemplo, calcular algum tipo de índice que considere o número de trabalhos e o número de programas (taxa de trabalhos por programa, por exemplo), e avaliar se esse padrão ainda permanece.\nNota: eu poderia ter utilizado a coluna de região que já vem no resultado da função read_state() – o que provavelmente teria diminuído o código pela metade –, mas infelizmente só notei isso à posteriori.",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Lista II</span>"
    ]
  },
  {
    "objectID": "listas/lista-2/lista2.html#produção-por-programa",
    "href": "listas/lista-2/lista2.html#produção-por-programa",
    "title": "10  Lista II",
    "section": "10.5 5. Produção por programa",
    "text": "10.5 5. Produção por programa\nAgora vamos analisar a produção por programa, isto é, o número de teses e dissertações defendidas. Primeiro, vamos lembrar de verificar se o tipo do programa (mestrado ou doutorado) está padronizado:\n\ntable(teses_dissertacoes_flt$nivel)\n\n\n            Doutorado             DOUTORADO              Mestrado \n                   18                    43                    29 \n             MESTRADO MESTRADO PROFISSIONAL    Profissionalizante \n                   84                    10                     3 \n\n\nÉ, infelizmente não é bem o caso. Vamos padronizar deixando tudo em lower case e transformar “MESTRADO PROFISSIONAL” simplesmente em “mestrado”, ignorando eventuais diferenças.\nTambém vamos fazer algumas modificações nome do programa: na verdade, apenas buscarei torná-lo mais informativo juntando-o ao nome da instituição de ensino. Veja: os nomes dos programas se diferenciam pouco e são inconsistentes – por exemplo, “ciencia politica”, “ciência política” e assim por diante. Vamos tratar essas inconsistências e juntá-las ao nome da universidade que coordena o programa.\n\nteses_dissertacoes_flt &lt;- teses_dissertacoes_flt %&gt;%\n  mutate(\n    nivel = str_to_lower(nivel),\n    nivel = case_when(\n      str_detect(nivel, \"mestrado\") ~ \"mestrado\",\n      str_detect(nivel, \"doutorado\") ~ \"doutorado\",\n      TRUE ~ nivel\n    ),\n    # essa parte foi co-authored com o chatgpt\n    nome_programa = nome_programa %&gt;%\n      str_to_lower() %&gt;% # minúsculas\n      str_trim() %&gt;% # remove espaços nas bordas\n      str_squish() %&gt;% # remove espaços duplicados no meio\n      stri_trans_general(\"Latin-ASCII\"), # remove acentos\n    nome_programa_ies = str_c(sigla_ies, \" - \", nome_programa)\n  )\n\n# dissertacoes\ndissertacoes_por_programa &lt;- teses_dissertacoes_flt %&gt;%\n  filter(nivel == \"mestrado\") %&gt;%\n  group_by(nome_programa_ies) %&gt;%\n  summarise(\n    n_dissertacoes = n()\n  )\n\n# teses\nteses_por_programa &lt;- teses_dissertacoes_flt %&gt;%\n  filter(nivel == \"doutorado\") %&gt;%\n  group_by(nome_programa_ies) %&gt;%\n  summarise(\n    n_teses = n()\n  )\n\n# tabela agrupada\nteses_dissertacoes_por_programa &lt;- full_join(\n  x=dissertacoes_por_programa,\n  y=teses_por_programa,\n  by=\"nome_programa_ies\"\n  )\n\n# apresentacao da tabela final bonitinha\nteses_dissertacoes_por_programa %&gt;%\n  arrange(desc(n_teses)) %&gt;%\n  head(10) %&gt;%\n  gt() %&gt;%\n  cols_label(\n    nome_programa_ies = \"Programa\",\n    n_dissertacoes = \"Dissertações\",\n    n_teses = \"Teses\"\n  ) %&gt;%\n  tab_header(\n    title = \"Produção Acadêmica por Programa\",\n    subtitle = \"Número de dissertações e teses por programa\"\n  ) %&gt;%\n  tab_options(\n    table.font.size = \"small\",\n    row.striping.include_table_body = TRUE\n  )\n\n\n\n\n\n\n\nProdução Acadêmica por Programa\n\n\nNúmero de dissertações e teses por programa\n\n\nPrograma\nDissertações\nTeses\n\n\n\n\nUFRGS - ciencia politica\n2\n7\n\n\nUFMG - ciencia politica\n4\n5\n\n\nUFSCAR - ciencia politica\n2\n5\n\n\nPUC/SP - ciencias sociais\n3\n3\n\n\nUNB - ciencia politica\n5\n3\n\n\nUSP - ciencia politica\n6\n3\n\n\nPUC-RIO - ciencias sociais\n2\n2\n\n\nPUC-RIO - relacoes internacionais\n1\n2\n\n\nUERJ - ciencia politica\n2\n2\n\n\nUFPE - sociologia\n1\n2\n\n\n\n\n\n\n\nObservamos, aqui, que os três principais programas (em termos de número de teses publicadas) que citam esses termos são os de Ciência Política da UFRGS, UFMG e UFSCAR.",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Lista II</span>"
    ]
  },
  {
    "objectID": "listas/lista-2/lista2.html#visualização-da-produção",
    "href": "listas/lista-2/lista2.html#visualização-da-produção",
    "title": "10  Lista II",
    "section": "10.6 6. Visualização da produção",
    "text": "10.6 6. Visualização da produção\nPor fim, vamos visualizar a produção dos programas. Basta agruparmos por ano, nome do programa, nível e conceito (que é o mesmo para todas as entradas agrupadas, então entra no agrupamento apenas para aparecer no dataset final), e, depois, pivotar a tabela para obter o número de dissertações e teses nas colunas.\n\nproducao &lt;- teses_dissertacoes_flt %&gt;%\n  filter(nivel %in% c(\"mestrado\", \"doutorado\")) %&gt;%\n  group_by(ano, nome_programa_ies, nivel, CONCEITO) %&gt;%\n  summarise(n = n(), .groups = \"drop\") %&gt;%\n  pivot_wider(\n    names_from = nivel,\n    values_from = n,\n    values_fill = 0\n  )\n\nApresento abaixo um scatterplot com jitter que, na verdade, acaba sendo uma visualização sub-ótima na medida em que, quando agrupamos por ano, o \\(n\\) de teses defendidas fica muito pequeno.\n\nproducao %&gt;%\n  drop_na() %&gt;%\n  ggplot(aes(x = as.factor(doutorado), y = CONCEITO)) +\n  geom_jitter(width = 0.15, height = 0.1, color = \"steelblue\", size = 3, alpha = 0.7) +\n  labs(\n    title = \"Relação entre teses defendidas e conceito CAPES\",\n    x = \"Teses defendidas\",\n    y = \"Conceito CAPES\",\n    caption = \"Feito por Felipe Lamarca usando dados da CAPES\"\n  ) +\n  theme_bw(base_size = 12) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    axis.title = element_text(face = \"bold\"),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\n\n\n\n\nIgnorando o ano e agrupando pelos conceitos, vemos que a maior parte das teses cujos resumos citam as palavras-chave indicadas foram defendidas por programas de pós-graduação com conceito 5.\n\nproducao %&gt;%\n  drop_na() %&gt;%\n  group_by(CONCEITO) %&gt;%\n  summarise(n_teses = sum(doutorado), .groups = \"drop\") %&gt;%\n  ggplot(aes(x = as.factor(CONCEITO), y = n_teses)) +\n  geom_col(fill = \"steelblue\") +\n  labs(\n    title = \"Teses defendidas por conceito CAPES\",\n    x = \"Conceito CAPES\",\n    y = \"Número de Teses\",\n    caption = \"Feito por Felipe Lamarca usando dados da CAPES\"\n  ) +\n  theme_bw(base_size = 12) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 16),\n    axis.title = element_text(face = \"bold\"),\n    axis.text.x = element_text(size = 11),\n    panel.grid.minor = element_blank()\n  )",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Lista II</span>"
    ]
  },
  {
    "objectID": "listas/lista-2/lista2.html#exportação",
    "href": "listas/lista-2/lista2.html#exportação",
    "title": "10  Lista II",
    "section": "10.7 7. Exportação",
    "text": "10.7 7. Exportação\nFinalmente, exportamos a base para análises futuras.\n\nteses_dissertacoes_flt %&gt;%\n  select(ano, UF, codigo_programa, nome_programa_ies, titulo, resumo, autor) %&gt;%\n  writexl::write_xlsx(\"data/teses_dissertacoes_flt.xlsx\")",
    "crumbs": [
      "Listas",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Lista II</span>"
    ]
  }
]