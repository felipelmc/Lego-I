# PrediÃ§Ã£o

## Llaudet, E. and Imai, K. (2022). _Data analysis for social science: A friendly and practical introduction_. Princeton University Press. Chapter 4: Predicting outcomes using linear regression.

> In the social sciences, we often are unable to observe the value of a particular variable of interest, $Y$, either because it hasn't occurred yet or because it is difficult to measure. In these situations, we typically observe the values of the other variable that, if correlated with $Y$, can be used to predict $Y$. (p. 99)

> When analyzing data for predictive purposes, then, we do not assume that there is a causal relationship between $X$ and $Y$; we simply rely on a high degree of correlation between them and use one variable to estimate the value of the other. (p. 99)

Primeiro usamos os dados disponÃ­veis para ajustar (_fit_) um modelo aos dados. Depois, quando nÃ£o podemos mais observar os valores da variÃ¡vel dependente (ou variÃ¡vel resposta), usamos o modelo ajustado para estimar os valores de $\hat{Y}$.

O modelo linear Ã© dado por:

$$
Y_i = \alpha + \beta X_i + \epsilon_i,
$$

onde:

- $Y_i$ Ã© a variÃ¡vel dependente (ou variÃ¡vel resposta);
- $X_i$ Ã© a variÃ¡vel independente (ou preditora);
- $\alpha$ Ã© o intercepto;
- $\beta$ Ã© o coeficiente angular (ou inclinaÃ§Ã£o);
- $\epsilon_i$ Ã© o erro aleatÃ³rio (ou resÃ­duo) associado Ã  observaÃ§Ã£o $i$.
- $i$ Ã© o Ã­ndice da observaÃ§Ã£o.

> Unfortunately, we do not know the values of $alpha$, $\beta$, and $\epsilon_i$. We need to estimate them based on data. We start by estimating the intercept ($\alpha$) and the slope ($\beta$), the two coefficients that define the line. This is equivalent to fitting a line to the data, that is, finding the line that best summarizes the relationship between $X$ and $Y$. (p. 102)

A reta ajustada Ã© dada por:

$$
\hat{Y}_i = \hat{\alpha} + \hat{\beta} X_i,
$$

onde:
- $\hat{Y}_i$ Ã© o valor predito da variÃ¡vel dependente (ou variÃ¡vel resposta);
- $\hat{\alpha}$ Ã© o valor estimado do intercepto;
- $\hat{\beta}$ Ã© o valor estimado do coeficiente angular (ou inclinaÃ§Ã£o);
- $X_i$ Ã© o valor da variÃ¡vel independente (ou preditora);

> Note that the value of $\hat{Y}$ produced by a fitted model is an _average_ predicted value; it is the average predicted value of $Y$ associated with a particular value of $X$. Indeed, predicted outcomes ($\hat{Y}$) are equivalent to average outcomes ($\bar{Y}$).

O erro estimado Ã© simplesmente a diferenÃ§a entre o valor observado e o valor predito:

$$
\hat{\epsilon}_i = Y_i - \hat{Y}_i = Y_i - (\hat{\alpha} + \hat{\beta} X_i).
$$

Desejamos sempre encontrar a reta que minimiza a soma dos quadrados dos erros.

### Os coeficientes

- o **intercepto** especifica a posiÃ§Ã£o da reta no eixo vertical. Mais especificamente, o intercepto Ã© o valor de $Y$ quando $X = 0$;
- o **coeficiente angular** especifica a inclinaÃ§Ã£o da reta. Mais especificamente, o coeficiente angular Ã© a variaÃ§Ã£o mÃ©dia de $Y$ associada a uma variaÃ§Ã£o unitÃ¡ria de $X$. Quando $\delta X = 1$, a variaÃ§Ã£o mÃ©dia de $Y$ Ã© $\hat{\beta}$.

### OLS: Ordinary Least Squares

> Formally, to choose the line of best fit, we use the "least squares" method, which identifies the line that minimizes the "sum of the squared residuals", known as SSR. 

$$
\text{SSR} = \sum_{i=1}^{n} = \hat{\epsilon}_i^2 = \sum_{i=1}^{n} (Y_i - \hat{Y}_i)^2 = \sum_{i=1}^{n} (Y_i - (\hat{\alpha} + \hat{\beta} X_i))^2
$$

> Why do we want to minimize the sum of the _squared_ residuals rather than the sum of the residuals? Because in the minimization process we want to avoid having positive prediction errors cancel out negative prediction errors. By squaring the residuals, we convert them all to positive numbers. (p. 107)

No exemplo do livro, os autores ajustam um modelo para prever o GDP dos paÃ­ses a partir da emissÃ£o de luzes noturnas. Mas, antes, podemos simplesmente ajustar um modelo que estima o GDP (2005-2006) atual a partir do GDP "anterior" (1992-1993):

$$
\hat{\text{gdp}}_{i} = \hat{\alpha} + \hat{\beta} \cdot \text{prior_gdp}_{i}
$$

Estimado o modelo, terÃ­amos os coeficientes:

$$
\hat{\text{gdp}}_{i} = 0.72 + 1.61 \cdot \text{prior_gdp}_{i}
$$

Suponha que, hÃ¡ alguns anos atrÃ¡s, o GDP do Brasil era de 400 trihÃµes de dÃ³lares. O GPD estimado seria:

$$
\hat{\text{gdp}}_{\text{BR}} = 0.72 + 1.61 \cdot 400 = 644.72
$$

### With natural logarithm transformations

> When a variable contains a handful of either extremely large or extremely small values, the distribution of the variable will be skewed. (Recall that a distribution is considered skewed when it is not symmetric because one of its tails is longer than the other.) Under these circumstances, it is often a good idea to transform the variable by taking its natural logarithm. This transformation will make the variable of interest more normally distributed and, in turn, improve the fit of the line to the data. In the example at hand, we will transform both variables of interest by taking the natural logarithm, and then we will re-fit the line. (p. 113)

> This type of model, in which both the outcome and the predictor have been log-transformed, is called the log-log linear model. While we could interpret the coefficients the same way as in the normal linear model, in practice, we use an approximation to avoid dealing with the logarithms, especially when interpreting $\hat{\beta}$. (p. 116)
>
> As shownn in the apprendix near the end of this chapter, we interpret $\hat{\beta}$ as the predicted percentage change in the outcome associated with an increase in the predictor of 1 percent. Since here $\hat{\beta} = 1.01$, an increase of prior GPD of 1 percent is associated with an increase in GPD of 1.01% on average. Note that in this interpretation of $\hat{\beta}$, both the change in $X$ and the change in $\hat{Y}$ are measured in percentages, instead of in units, as is the case in the standard linear model. In other words, in the log-log model, we estimate change in relative rather than in absolute terms. (p. 116)

No apÃªndice do capÃ­tulo, os autores discutem matematicamente a interpretaÃ§Ã£o de $\hat{\beta}$ no modelo log-log. A fÃ³rmula para a variaÃ§Ã£o percentual de $Y$ associada a uma variaÃ§Ã£o percentual de $X$ Ã© dada por:

$$
\begin{align*}
\hat{\log(Y_\text{final})} - \hat{\log(Y_\text{inicial})} &= [\hat{\alpha} + \hat{\beta} \cdot \log(X_\text{final})] - [\hat{\alpha} + \hat{\beta} \cdot \log(X_\text{inicial})] \\
&= \hat{\beta} \cdot [\log(X_\text{final}) - \log(X_\text{inicial})] \\
\end{align*}
$$

Devemos usar a seguinte aproximaÃ§Ã£o:

$$
\log(Y_\text{final}) - \log(Y_\text{inicial}) = \log \left( \frac{Y_\text{final} - Y_\text{inicial}}{Y_\text{inicial}} \right) \approx \frac{\Delta Y}{Y_\text{inicial}}
$$

Isso Ã© verdade porque, para pequenas variaÃ§Ãµes, a diferenÃ§a entre o logaritmo de dois nÃºmeros Ã© aproximadamente igual Ã  razÃ£o entre a diferenÃ§a e o nÃºmero inicial. Em particular, a seguinte afirmaÃ§Ã£o Ã© vÃ¡lida:

$$
\log(Y + \Delta Y) - \log(Y) \approx \frac{\Delta Y}{Y}
$$

No caso acima, estamos reescrevendo $\log(Y_\text{final})$ como $\log(Y + \Delta Y)$ Essa Ã© uma propriedade do logaritmo natural derivada da expansÃ£o em sÃ©rie de Taylor:

$$
\log (1+\epsilon) \approx \epsilon - \frac{\epsilon^2}{2} + \frac{\epsilon^3}{3} - \cdots \approx \epsilon
$$

onde $\epsilon$ Ã© um nÃºmero pequeno. Portanto, podemos usar a seguinte aproximaÃ§Ã£o:

Entendemos, com isso, que a variaÃ§Ã£o percentual de $Y$ associada a uma variaÃ§Ã£o percentual de $X$ Ã© dada por:

$$
\frac{\Delta Y}{Y_\text{inicial}} \times 100 \approx \hat{\beta} \cdot \frac{\Delta X}{X_\text{inicial}} \times 100
$$

Matematicamente, escrevemos:

$$
\begin{align*}
[\log(X_\text{final}) - \log(X_\text{inicial})] \cdot 100 &= [\log(X_\text{inicial} + \Delta X) - \log(X_\text{inicial})] \cdot 100 \\
&= [\log (X_\text{inicial} + X_\text{inicial} \frac{\Delta X}{X_\text{inicial}}) - \log(X_\text{inicial})] \cdot 100 \\
&= [\log (X_\text{inicial} (1 + \frac{\Delta X}{X_\text{inicial}})) - \log(X_\text{inicial})] \cdot 100 \\
&= [\log (1 + \frac{\Delta X}{X_\text{inicial}})] \cdot 100 \\
&\approx \frac{\Delta X}{X_\text{inicial}} \cdot 100
\end{align*}
$$

### Measuring how well the model fits the data with the coefficient of determination, $R^2$

> The value of $R^2$ ranges from 0 to 1 and represents the proportion of the variation of Y explained by the model. [...]. Therefore, the higher the $R^2$, the better the model fits the data. (p. 120)

$$
R^2 = 1 - \frac{\text{SSR}}{\text{TSS}}
$$

onde:
- $SSR$ Ã© a soma dos quadrados dos resÃ­duos;
- $TSS$ Ã© a soma total dos quadrados, que mede a variaÃ§Ã£o total de $Y$ em relaÃ§Ã£o Ã  mÃ©dia de $Y$:

$$
TSS = \sum_{i=1}^{n} (Y_i - \bar{Y})^2
$$

No fim das contas, TSS Ã© simplesmente o numerador da variÃ¢ncia.

> Given the definitions above, we can interpret SSR/TSS as the proportion of the variation of $Y$ not explained by the model. Therefore, 1-SSR/TSS is the proportion of the variation of $Y$ that is explained by the model. (p. 121)

ðŸ’¡A interpretaÃ§Ã£o de que o SSR Ã© a variaÃ§Ã£o nÃ£o explicada Ã© simples: como se trata da diferenÃ§a entre o que o modelo diz e o que de fato Ã© o valor observado, entendemos que o modelo falhou em explicar essa variaÃ§Ã£o. AlÃ©m disso, como o TSS representa a variaÃ§Ã£o total em relaÃ§Ã£o Ã  mÃ©dia, podemos usÃ¡-lo como denominador.

## Verhagen, M. D. (2022) A pragmatist's guide to using prediction in the social sciences. _Socius_, 8, 23780231221081702

> The current lack of prediction in the social sciences stems from a seeming incompatibility between wanting to explain and wanting to predict, effectively forcing researchers to choose between the two approaches. A case in point is the much-cited article by Galit Shmueli (2010), aptly titled "To Predict or to Explain", which outlines how a social scientist's empirical work flow differs in terms of data processing, modeling, and postestimation diagnostics when choosing to either predict or explain. Naturally, Shmueli assumes that a researcher would not normally attempt to do both. This is an accurate reflection of social science research. The apparent need to dogmatically choose between either approach means that, in practice, social scientists tend to stick to explanation almost exclusively. (p. 1)

> When viewing prediction as a simple tool to evaluate a model's ability to approximate the outcome of interest, it can be applied without exception to most social science questions, rendering a dogmatic choice between prediction or explanation unnecessary. (p. 2)

O autor faz uma discussÃ£o sobre possÃ­veis approaches para a prediÃ§Ã£o -- in-sample evaluation, k-fold cross validation, e external evaluation.

AlÃ©m disso, discute tambÃ©m a importÃ¢ncia da prediÃ§Ã£o em relaÃ§Ã£o Ã s informaÃ§Ãµes que ela pode trazer sobre os modelos ajustados. Em particular, hÃ¡ 3 virtudes principais:

1. A prediÃ§Ã£o dÃ¡ informaÃ§Ãµes sobre o ajuste do modelo aos dados;
2. A prediÃ§Ã£o permite o estabelecimento de _benchmarks_ para a avaliaÃ§Ã£o de modelos em diferentes domÃ­nios;
3. A prediÃ§Ã£o pode ser usada para entender melhor modelos complicados.